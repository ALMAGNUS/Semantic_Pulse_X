# üéØ GUIDE DE D√âMONSTRATION - Semantic Pulse X
## Pour le Professeur - Session Pratique

---

## ‚è±Ô∏è **PLAN DE D√âMONSTRATION (15 minutes)**

### **Phase 1 : Architecture (3 min)**
```bash
# Montrer la structure du projet
tree -L 3
ls -la scripts/
```

**Points √† souligner** :
- 5 sources de donn√©es distinctes
- Scripts modulaires et ind√©pendants
- Architecture MERISE respect√©e

---

### **Phase 2 : Test de Conformit√© (2 min)**
```bash
# Test automatique complet
python scripts/test_components_individual.py
```

**R√©sultat attendu** : 10/10 tests pass√©s ‚úÖ

---

### **Phase 3 : Pipeline ETL (3 min)**
```bash
# 1. Collecte des donn√©es
python scripts/scrape_yahoo.py
python scripts/scrape_franceinfo_selenium.py

# 2. Agr√©gation MERISE
python scripts/aggregate_sources.py --input-dir data/raw --output-file data/processed/demo_prof.json --min-text-len 5

# 3. Chargement en base
python scripts/load_aggregated_to_db.py --input data/processed/demo_prof.json
```

**Points √† souligner** :
- Collecte automatique multi-sources
- Normalisation des donn√©es
- Int√©gration MERISE

---

### **Phase 4 : Big Data GDELT (2 min)**
```bash
# Pipeline Big Data
python scripts/gdelt_gkg_pipeline.py --days 1 --output data/processed/gdelt_demo.json
```

**Points √† souligner** :
- Source Big Data d√©di√©e (pas de d√©coupage Kaggle)
- Traitement de gros volumes
- Int√©gration avec le pipeline principal

---

### **Phase 5 : Interface Utilisateur (3 min)**
```bash
# Lancement Streamlit
streamlit run app/frontend/streamlit_app.py
```

**D√©monstration** :
- Dashboard temps r√©el
- Visualisations interactives
- Analyse d'√©motions
- Word Cloud

---

### **Phase 6 : Base de Donn√©es (2 min)**
```bash
# V√©rification de la base MERISE
sqlite3 semantic_pulse.db "SELECT COUNT(*) FROM contenus;"
sqlite3 semantic_pulse.db "SELECT nom, type, COUNT(*) FROM sources GROUP BY nom, type;"
```

**Points √† souligner** :
- Sch√©ma MERISE respect√©
- Cardinalit√©s correctes
- Donn√©es int√©gr√©es et coh√©rentes

---

## üéØ **POINTS CL√âS √Ä SOULIGNER**

### **1. Conformit√© E1/E2/E3**
- ‚úÖ **E1** : 5 sources distinctes (fichier plat, DB relationnelle, API externe, web scraping, Big Data)
- ‚úÖ **E2** : Architecture MERISE compl√®te avec pipeline ETL robuste
- ‚úÖ **E3** : Fonctionnalit√©s cr√©atives (pr√©diction d'√©motions, IA fran√ßaise)

### **2. Innovation Technique**
- **Web Scraping r√©el** avec Selenium (exigence professeur)
- **Big Data GDELT** (pas de d√©coupage artificiel)
- **Pipeline ETL modulaire** et ind√©pendant
- **IA fran√ßaise** pour analyse d'√©motions

### **3. Qualit√© du Code**
- **Scripts ind√©pendants** et r√©utilisables
- **Gestion d'erreurs** robuste
- **Documentation** compl√®te
- **Tests automatis√©s** valid√©s

---

## üîß **COMMANDES DE D√âMONSTRATION RAPIDE**

### **Test Complet (30 secondes)**
```bash
python scripts/test_components_individual.py
```

### **Pipeline ETL Complet (2 minutes)**
```bash
python scripts/aggregate_sources.py --input-dir data/raw --output-file data/processed/demo.json --min-text-len 5
python scripts/load_aggregated_to_db.py --input data/processed/demo.json
```

### **Interface Utilisateur (1 minute)**
```bash
streamlit run app/frontend/streamlit_app.py
```

---

## üìä **M√âTRIQUES √Ä PR√âSENTER**

### **Donn√©es Int√©gr√©es**
- **YouTube** : 180 vid√©os avec texte complet
- **Web Scraping** : Articles Yahoo + France Info
- **GDELT** : 1,283 enregistrements Big Data
- **Total** : 535 contenus analys√©s

### **Performance**
- **Test end-to-end** : Score 150%
- **Pipeline ETL** : 100% fonctionnel
- **Code quality** : 0 erreurs Ruff
- **Architecture** : MERISE compl√®te

---

## üéì **R√âPONSES AUX QUESTIONS PROBABLES**

### **Q: Comment garantissez-vous la conformit√© MERISE ?**
**R:** 
- Sch√©ma ORM g√©n√©r√© automatiquement (`generate_orm_schema.py`)
- Cardinalit√©s respect√©es dans les relations
- Base SQLite avec contraintes d'int√©grit√©
- Documentation Mermaid mise √† jour

### **Q: Quelle est votre source Big Data ?**
**R:** 
- GDELT GKG (Global Knowledge Graph)
- Donn√©es g√©opolitiques mondiales
- Pas de d√©coupage artificiel du dataset Kaggle
- Pipeline d√©di√© (`gdelt_gkg_pipeline.py`)

### **Q: Comment g√©rez-vous le web scraping ?**
**R:** 
- Selenium pour JavaScript (exigence professeur)
- Requests + BeautifulSoup pour HTML simple
- Respect robots.txt
- Gestion des encodages et erreurs

### **Q: Votre pipeline ETL est-il robuste ?**
**R:** 
- Scripts modulaires et ind√©pendants
- Gestion des donn√©es manquantes
- Tests automatis√©s complets
- Gestion d'erreurs √† tous les niveaux

---

## üöÄ **D√âMONSTRATION FINALE**

### **Commande Magique (Tout en une fois)**
```bash
# Test + Pipeline + Interface
python scripts/test_components_individual.py && \
python scripts/aggregate_sources.py --input-dir data/raw --output-file data/processed/final_demo.json --min-text-len 5 && \
python scripts/load_aggregated_to_db.py --input data/processed/final_demo.json && \
echo "‚úÖ Pipeline complet termin√© ! Lancement de l'interface..." && \
streamlit run app/frontend/streamlit_app.py
```

**R√©sultat** : D√©monstration compl√®te en 5 minutes !

---

*Guide pr√©par√© pour la pr√©sentation professeur - Janvier 2025*
