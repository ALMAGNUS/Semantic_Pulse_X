# ü§ñ Ollama - Mod√®les IA Locaux pour Semantic Pulse X

## üéØ Pourquoi Ollama ?

**Ollama** est la solution IA g√©n√©rative **gratuite et la plus efficace** pour Semantic Pulse X :

### ‚úÖ **Avantages cl√©s :**
- **100% gratuit** - Aucun co√ªt d'API
- **RGPD-compliant** - Donn√©es restent sur votre serveur
- **Performance excellente** - Optimis√© pour la production
- **Mod√®les fran√ßais** - Mistral, Llama 2, CodeLlama
- **Facile √† int√©grer** - API REST simple
- **Ressources contr√¥l√©es** - Pas de d√©pendance externe

## üöÄ Installation rapide

### 1. **D√©marrage d'Ollama**
```bash
# D√©marrer tous les services (Ollama inclus)
docker-compose up -d

# V√©rifier le statut d'Ollama
curl http://localhost:11434/api/tags

# V√©rifier tous les services
docker-compose ps
```

### 2. **Configuration automatique**
```bash
# Installer et configurer les mod√®les
python scripts/setup_ollama.py

# Ou en mode Docker (si Ollama est dans un conteneur)
docker-compose exec app python scripts/setup_ollama.py
```

### 3. **Test de l'installation**
```bash
# Test rapide
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "mistral:7b", "prompt": "Bonjour!"}'
```

## üìä Mod√®les recommand√©s

| Mod√®le | Taille | Usage | Performance |
|--------|--------|-------|-------------|
| **mistral:7b** | 7B | Mod√®le principal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **llama2:7b** | 7B | Mod√®le alternatif | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **codellama:7b** | 7B | G√©n√©ration de code | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **phi3:3.8b** | 3.8B | Mod√®le l√©ger | ‚≠ê‚≠ê‚≠ê |

## üéõÔ∏è Interface de gestion

### **Dashboard Streamlit**
- **URL** : http://localhost:8501 (onglet "ü§ñ Ollama IA")
- **Fonctionnalit√©s** :
  - Gestion des mod√®les
  - Tests en temps r√©el
  - Chat interactif
  - Configuration avanc√©e

### **API REST**
```python
from app.ai.ollama_client import ollama_client

# G√©n√©ration de texte
response = ollama_client.generate_text(
    prompt="Analysez les tendances √©motionnelles",
    model="mistral:7b",
    temperature=0.7
)

# Analyse des √©motions
insights = ollama_client.analyze_emotion_trends(emotion_data)
```

## üîß Configuration avanc√©e

### **Variables d'environnement**
```env
# Ollama
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=mistral:7b
```

### **Configuration GPU (optionnel)**
```yaml
# docker-compose.ollama.yml
services:
  ollama:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## üìà Utilisation dans Semantic Pulse X

### **1. Analyse √©motionnelle**
```python
# Analyse des tendances
trends = ollama_client.analyze_emotion_trends(emotion_data)

# G√©n√©ration d'insights
insights = ollama_client.generate_insights(data)
```

### **2. D√©tection d'anomalies**
```python
# D√©tection automatique
anomalies = ollama_client.detect_anomalies(data)

# G√©n√©ration d'alertes
alert = ollama_client.generate_alert(anomaly)
```

### **3. Chat interactif**
```python
# R√©ponses contextuelles
answer = ollama_client.answer_question(question, context)
```

## üõ†Ô∏è D√©pannage

### **Probl√®mes courants**

#### Ollama ne d√©marre pas
```bash
# V√©rifier les logs
docker-compose logs ollama

# Red√©marrer Ollama
docker-compose restart ollama

# Red√©marrer tous les services
docker-compose restart
```

#### Mod√®les non disponibles
```bash
# Lister les mod√®les
curl http://localhost:11434/api/tags

# T√©l√©charger un mod√®le
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "mistral:7b"}'
```

#### Performance lente
```bash
# V√©rifier les ressources
docker stats semantic-pulse-ollama

# Optimiser la m√©moire
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_LOADED_MODELS=1
```

## üìä Monitoring

### **M√©triques disponibles**
- **Temps de r√©ponse** - Latence des requ√™tes
- **Utilisation m√©moire** - Consommation RAM
- **Mod√®les actifs** - Nombre de mod√®les charg√©s
- **Requ√™tes par minute** - Volume de trafic

### **Logs**
```bash
# Logs Ollama
docker logs semantic-pulse-ollama

# Logs de l'application
tail -f logs/semantic_pulse.log
```

## üîÑ Mise √† jour

### **Mise √† jour d'Ollama**
```bash
# Mettre √† jour l'image
docker-compose pull ollama

# Red√©marrer Ollama
docker-compose up -d ollama

# Ou red√©marrer tous les services
docker-compose up -d
```

### **Mise √† jour des mod√®les**
```bash
# Mettre √† jour un mod√®le
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "mistral:7b", "stream": false}'
```

## üÜò Support

### **Documentation**
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Mod√®les disponibles](https://ollama.ai/library)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)

### **Communaut√©**
- [Discord Ollama](https://discord.gg/ollama)
- [GitHub Issues](https://github.com/ollama/ollama/issues)

### **Semantic Pulse X**
- **Issues** : https://github.com/your-username/semantic-pulse-x/issues
- **Email** : support@semantic-pulse.com

## üéØ Prochaines √©tapes

1. **Installer tous les services** avec `docker-compose up -d`
2. **Configurer les mod√®les** avec `python scripts/setup_ollama.py`
3. **Tester l'interface** sur http://localhost:8501
4. **Int√©grer dans vos workflows** Semantic Pulse X

**Ollama est maintenant int√©gr√© dans Semantic Pulse X !** üöÄ
