# ğŸš€ Technologies Big Data - Semantic Pulse X

## ğŸ“Š **Parquet - Format de stockage optimisÃ©**

### Qu'est-ce que Parquet ?
Parquet est un format de fichier colonnaire open-source optimisÃ© pour l'analytique de donnÃ©es volumineuses.

### Avantages clÃ©s :
- **Compression** : 80-90% de rÃ©duction de taille
- **Performance** : Lecture 10x plus rapide que CSV
- **CompatibilitÃ©** : SupportÃ© par tous les outils Big Data
- **Types complexes** : Dates, dÃ©cimaux, structures imbriquÃ©es

### Exemple concret dans notre projet :
```python
# Conversion CSV vers Parquet
CSV: 1.09 MB (10,000 tweets)
Parquet: 0.16 MB (85% de compression)
```

### Utilisation avec Pandas :
```python
import pandas as pd

# Lecture optimisÃ©e
df = pd.read_parquet('data/tweets.parquet')

# Ã‰criture avec compression
df.to_parquet('output.parquet', compression='snappy')
```

---

## ğŸ—„ï¸ **MinIO - Data Lake S3-compatible**

### Qu'est-ce que MinIO ?
MinIO est un serveur de stockage objet haute performance, compatible avec l'API Amazon S3.

### Architecture dans Semantic Pulse X :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources       â”‚    â”‚   MinIO         â”‚    â”‚   Analytics     â”‚
â”‚   (CSV/JSON)    â”‚â”€â”€â”€â–¶â”‚   Data Lake     â”‚â”€â”€â”€â–¶â”‚   (Polars/Duck) â”‚
â”‚                 â”‚    â”‚   (Parquet)     â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Organisation des buckets :
- **`semantic-pulse-raw/`** : DonnÃ©es brutes des 5 sources
- **`semantic-pulse-processed/`** : DonnÃ©es nettoyÃ©es et anonymisÃ©es
- **`semantic-pulse-analytics/`** : AgrÃ©gations et mÃ©triques

### Configuration Docker :
```yaml
minio:
  image: minio/minio:latest
  command: server /data --console-address ":9001"
  environment:
    MINIO_ROOT_USER: admin
    MINIO_ROOT_PASSWORD: admin123
  ports:
    - "9000:9000"  # API S3
    - "9001:9001"  # Interface web
```

### Utilisation Python :
```python
from minio import Minio

# Connexion
client = Minio('localhost:9000', 'admin', 'admin123', secure=False)

# Upload
client.fput_object('semantic-pulse-data', 'tweets.parquet', 'local_file.parquet')

# Download
client.fget_object('semantic-pulse-data', 'tweets.parquet', 'local_file.parquet')
```

---

## ğŸ”„ **Pipeline Big Data complet**

### 1. Collecte (5 sources)
- **Flat files** : CSV â†’ Parquet
- **Database** : PostgreSQL â†’ Parquet
- **Big Data** : Twitter dumps â†’ Parquet
- **Web scraping** : HTML â†’ JSON â†’ Parquet
- **REST API** : JSON â†’ Parquet

### 2. Stockage MinIO
```python
# Upload automatique
def upload_to_datalake(file_path: str, bucket: str):
    client.fput_object(bucket, os.path.basename(file_path), file_path)
```

### 3. Analytics avec Polars/DuckDB
```python
import polars as pl

# Lecture optimisÃ©e depuis MinIO
df = pl.read_parquet('s3://semantic-pulse-data/tweets.parquet')

# RequÃªtes SQL sur Big Data
result = pl.sql("""
    SELECT emotion, COUNT(*) as count
    FROM df
    WHERE date >= '2024-01-01'
    GROUP BY emotion
""")
```

---

## ğŸ“ˆ **MÃ©triques de performance**

### Compression des donnÃ©es :
| Format | Taille | Compression |
|--------|--------|-------------|
| CSV    | 1.09 MB | - |
| Parquet| 0.16 MB | 85% |

### Vitesse de lecture :
| Format | Temps de lecture |
|--------|------------------|
| CSV    | 2.3s |
| Parquet| 0.2s |

### Ã‰conomie d'espace totale :
- **Avant** : 5 sources Ã— 1MB = 5MB
- **AprÃ¨s** : 5 sources Ã— 0.15MB = 0.75MB
- **Ã‰conomie** : 85% d'espace disque

---

## ğŸ› ï¸ **Outils de dÃ©veloppement**

### Scripts disponibles :
- `scripts/convert_csv_to_parquet.py` : Conversion automatique
- `scripts/upload_to_minio.py` : Upload vers Data Lake
- `scripts/test_bigdata_pipeline.py` : Tests de performance

### Monitoring :
- **MinIO Console** : http://localhost:9001
- **MÃ©triques Prometheus** : Stockage, bande passante
- **Logs dÃ©taillÃ©s** : Upload/download, erreurs

---

## ğŸ¯ **Prochaines Ã©tapes**

1. âœ… **Conversion CSV â†’ Parquet** (TerminÃ©)
2. ğŸ”„ **Upload vers MinIO** (En cours)
3. â³ **IntÃ©gration Polars/DuckDB**
4. â³ **Analytics temps rÃ©el**
5. â³ **Monitoring avancÃ©**
