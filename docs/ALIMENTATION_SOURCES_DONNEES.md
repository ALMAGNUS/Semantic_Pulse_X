# ğŸ”— ALIMENTATION DES SOURCES DE DONNÃ‰ES - Semantic Pulse X

## ğŸ“Š **VUE D'ENSEMBLE**

**Semantic Pulse X** utilise **5 sources distinctes** qui alimentent une **6Ã¨me source agrÃ©gÃ©e (base MERISE)**. Voici comment chaque source est alimentÃ©e :

---

## 1ï¸âƒ£ **ğŸ“ FICHIER PLAT - 50% Kaggle Sentiment140**

### **Comment c'est alimentÃ© :**
- **Source :** Dataset Kaggle "Sentiment140" (1.6M tweets) â†’ **50% pour fichier plat**
- **Script :** `scripts/split_kaggle_dataset.py`
- **Processus :** 
  ```bash
  python scripts/split_kaggle_dataset.py
  ```
- **RÃ©sultat :** `data/raw/kaggle_tweets/file_source_tweets.csv` (3,333 tweets)

### **Contenu :**
- Tweets avec sentiment (positive/negative)
- Texte, date, utilisateur anonymisÃ©
- **Volume :** 3,333 tweets (50% du dataset Kaggle)

---

## 2ï¸âƒ£ **ğŸ—„ï¸ BASE DE DONNÃ‰ES SIMPLE - 50% Kaggle Sentiment140**

### **Comment c'est alimentÃ© :**
- **Source :** Dataset Kaggle "Sentiment140" â†’ **50% pour base simple**
- **Script :** `scripts/split_kaggle_dataset.py`
- **Processus :**
  ```bash
  python scripts/split_kaggle_dataset.py
  ```
- **RÃ©sultat :** `data/raw/kaggle_tweets/db_source_tweets.csv` (3,333 tweets)

### **Contenu :**
- Tweets avec sentiment (positive/negative)
- Texte, date, utilisateur anonymisÃ©
- **Volume :** 3,333 tweets (50% du dataset Kaggle)

---

## 3ï¸âƒ£ **ğŸ“ˆ SYSTÃˆME BIG DATA - GDELT GKG**

### **Comment c'est alimentÃ© :**
- **Script :** `scripts/gdelt_gkg_pipeline.py`
- **Processus :**
  ```bash
  python scripts/gdelt_gkg_pipeline.py --days 7 --output-dir data/raw
  ```
- **Source :** GDELT 2.0 Global Knowledge Graph
- **Technologie :** API REST + traitement Big Data
- **RÃ©sultat :** `data/raw/gdelt_data.json`

### **Contenu :**
- Ã‰vÃ©nements mondiaux avec gÃ©olocalisation
- ThÃ¨mes, acteurs, Ã©motions dÃ©tectÃ©es
- **Volume :** 1283+ Ã©vÃ©nements traitÃ©s

---

## 4ï¸âƒ£ **ğŸŒ API EXTERNE - YouTube + NewsAPI**

### **Comment c'est alimentÃ© :**

#### **YouTube Data API v3 :**
- **Script :** `scripts/collect_hugo_youtube.py`
- **Processus :**
  ```bash
  python scripts/collect_hugo_youtube.py
  ```
- **Configuration :** ClÃ© API YouTube dans `.env`
- **RÃ©sultat :** `data/raw/external_apis/hugo_*.json`
- **Contenu :** VidÃ©os Hugo Decrypte (titre, description, commentaires)

#### **NewsAPI :**
- **Script :** `scripts/collect_newsapi.py`
- **Processus :**
  ```bash
  python scripts/collect_newsapi.py --query "politique" --country "fr"
  ```
- **Configuration :** ClÃ© API NewsAPI dans `.env`
- **RÃ©sultat :** `data/raw/external_apis/newsapi_*.json`
- **Contenu :** Articles d'actualitÃ© franÃ§ais

### **Volume total :** 180 vidÃ©os + articles d'actualitÃ©

---

## 5ï¸âƒ£ **ğŸ•·ï¸ WEB SCRAPING - Yahoo + Franceinfo**

### **Comment c'est alimentÃ© :**

#### **Yahoo ActualitÃ©s FR :**
- **Script :** `scripts/scrape_yahoo.py`
- **Processus :**
  ```bash
  python scripts/scrape_yahoo.py --discover 1 --pays FR --domaine politique
  ```
- **Technologie :** requests + BeautifulSoup
- **RÃ©sultat :** `data/raw/scraped/yahoo_*.json`

#### **Franceinfo :**
- **Script :** `scripts/scrape_franceinfo_selenium.py`
- **Processus :**
  ```bash
  python scripts/scrape_franceinfo_selenium.py --discover 1 --pays FR --domaine politique
  ```
- **Technologie :** Selenium + Chrome WebDriver
- **RÃ©sultat :** `data/raw/scraped/franceinfo_*.json`

### **Contenu :**
- Articles d'actualitÃ© (titre, texte, date, URL)
- **Volume :** Articles collectÃ©s en temps rÃ©el

---

## 6ï¸âƒ£ **ğŸ”„ BASE AGRÃ‰GÃ‰E - semantic_pulse.db (Addition des 5 sources)**

### **Comment c'est alimentÃ© :**
- **Source :** **Addition des 5 sources prÃ©cÃ©dentes**
- **Script principal :** `scripts/aggregate_sources.py`
- **Processus complet :**
  ```bash
  # 1. Collecte des 5 sources
  python scripts/split_kaggle_dataset.py  # Sources 1+2 (Kaggle 50%+50%)
  python scripts/gdelt_gkg_pipeline.py --days 1  # Source 3 (Big Data)
  python scripts/collect_hugo_youtube.py  # Source 4 (API YouTube)
  python scripts/collect_newsapi.py  # Source 4 (API NewsAPI)
  python scripts/scrape_yahoo.py --discover 1  # Source 5 (Web Scraping)
  
  # 2. AgrÃ©gation et normalisation des 5 sources
  python scripts/aggregate_sources.py --input-dir data/raw --output-file data/processed/integrated.json
  
  # 3. Chargement en base MERISE (6Ã¨me source)
  python scripts/load_aggregated_to_db.py --input data/processed/integrated.json
  ```

### **Contenu final :**
- **535 contenus** intÃ©grÃ©s (addition des 5 sources)
- **487 sources** tracÃ©es
- **SchÃ©ma MERISE** complet avec cardinalitÃ©s
- **ConformitÃ© RGPD** (anonymisation, pseudonymisation)

---

## ğŸ”„ **PIPELINE ETL COMPLET**

```mermaid
graph TD
    A[Kaggle 50% - Fichier plat] --> E[aggregate_sources.py]
    B[Kaggle 50% - Base simple] --> E
    C[GDELT GKG - Big Data] --> E
    D[YouTube + NewsAPI] --> E
    F[Yahoo + Franceinfo - Web Scraping] --> E
    
    E --> G[load_aggregated_to_db.py]
    G --> H[semantic_pulse.db - Base MERISE]
    
    style E fill:#e8f5e8
    style G fill:#e3f2fd
    style H fill:#e0f2f1
```

---

## ğŸ¯ **POINTS CLÃ‰S POUR LE PROF**

1. **5 sources distinctes** â†’ Chacune a son script d'alimentation
   - **Source 1 :** 50% Kaggle â†’ Fichier plat CSV
   - **Source 2 :** 50% Kaggle â†’ Base de donnÃ©es simple
   - **Source 3 :** GDELT GKG â†’ SystÃ¨me Big Data
   - **Source 4 :** YouTube + NewsAPI â†’ API externe
   - **Source 5 :** Yahoo + Franceinfo â†’ Web Scraping
2. **Pipeline ETL** â†’ AgrÃ©gation automatique des 5 sources
3. **Base MERISE** â†’ SchÃ©ma relationnel avec cardinalitÃ©s (6Ã¨me source)
4. **ConformitÃ© RGPD** â†’ Anonymisation et traÃ§abilitÃ©
5. **Collecte dynamique** â†’ Boutons Streamlit pour lancer les scripts
6. **Volume rÃ©el** â†’ 535 contenus intÃ©grÃ©s dans la base finale

**Chaque source est alimentÃ©e par un script dÃ©diÃ© et traÃ§able !** âœ…
