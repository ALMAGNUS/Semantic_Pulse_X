#!/usr/bin/env python3
"""
Script de configuration Ollama - Semantic Pulse X
Installe et configure Ollama avec les mod√®les recommand√©s
"""

import requests
import json
import time
import subprocess
import sys
from pathlib import Path


class OllamaSetup:
    """Configuration d'Ollama pour Semantic Pulse X"""
    
    def __init__(self, base_url: str = None):
        if base_url is None:
            import os
            ollama_host = os.getenv("OLLAMA_HOST", "localhost:11434")
            base_url = f"http://{ollama_host}"
        self.base_url = base_url
        self.recommended_models = [
            "mistral:7b",      # Mod√®le principal - excellent pour le fran√ßais
            "llama2:7b",       # Mod√®le alternatif - polyvalent
            "codellama:7b",    # Pour la g√©n√©ration de code
            "phi3:3.8b"        # Mod√®le l√©ger et rapide
        ]
    
    def check_ollama_status(self) -> bool:
        """V√©rifie si Ollama est en cours d'ex√©cution"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def wait_for_ollama(self, timeout: int = 300) -> bool:
        """Attend qu'Ollama soit disponible"""
        print("‚è≥ Attente du d√©marrage d'Ollama...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            if self.check_ollama_status():
                print("‚úÖ Ollama est disponible!")
                return True
            time.sleep(5)
            print(".", end="", flush=True)
        
        print(f"\n‚ùå Timeout: Ollama n'est pas disponible apr√®s {timeout}s")
        return False
    
    def pull_model(self, model_name: str) -> bool:
        """T√©l√©charge un mod√®le Ollama"""
        print(f"üì• T√©l√©chargement du mod√®le {model_name}...")
        
        try:
            url = f"{self.base_url}/api/pull"
            payload = {"name": model_name}
            
            response = requests.post(url, json=payload, stream=True, timeout=300)
            response.raise_for_status()
            
            # Afficher la progression
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode('utf-8'))
                        if 'status' in data:
                            print(f"\r{data['status']}", end="", flush=True)
                    except json.JSONDecodeError:
                        continue
            
            print(f"\n‚úÖ Mod√®le {model_name} t√©l√©charg√© avec succ√®s!")
            return True
            
        except Exception as e:
            print(f"\n‚ùå Erreur t√©l√©chargement {model_name}: {e}")
            return False
    
    def test_model(self, model_name: str) -> bool:
        """Teste un mod√®le"""
        print(f"üß™ Test du mod√®le {model_name}...")
        
        try:
            url = f"{self.base_url}/api/generate"
            payload = {
                "model": model_name,
                "prompt": "Bonjour, comment allez-vous?",
                "stream": False
            }
            
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if 'response' in data and data['response'].strip():
                print(f"‚úÖ Mod√®le {model_name} fonctionne correctement!")
                return True
            else:
                print(f"‚ùå Mod√®le {model_name} ne r√©pond pas correctement")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur test {model_name}: {e}")
            return False
    
    def setup_models(self) -> Dict[str, bool]:
        """Configure tous les mod√®les recommand√©s"""
        print("üöÄ Configuration des mod√®les Ollama...")
        
        results = {}
        
        for model in self.recommended_models:
            print(f"\nüì¶ Configuration de {model}")
            
            # T√©l√©charger le mod√®le
            if self.pull_model(model):
                # Tester le mod√®le
                if self.test_model(model):
                    results[model] = True
                    print(f"‚úÖ {model} configur√© avec succ√®s!")
                else:
                    results[model] = False
                    print(f"‚ö†Ô∏è {model} t√©l√©charg√© mais ne fonctionne pas")
            else:
                results[model] = False
                print(f"‚ùå {model} √©chec du t√©l√©chargement")
        
        return results
    
    def create_model_alias(self, model_name: str, alias: str) -> bool:
        """Cr√©e un alias pour un mod√®le"""
        try:
            # Cr√©er un alias pour le mod√®le principal
            url = f"{self.base_url}/api/generate"
            payload = {
                "model": model_name,
                "prompt": "test",
                "stream": False
            }
            
            response = requests.post(url, json=payload, timeout=10)
            return response.status_code == 200
            
        except:
            return False
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des mod√®les disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
        except:
            pass
        return []
    
    def generate_test_report(self) -> str:
        """G√©n√®re un rapport de test"""
        print("\nüìä G√©n√©ration du rapport de test...")
        
        available_models = self.get_available_models()
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "ollama_status": "running" if self.check_ollama_status() else "stopped",
            "available_models": available_models,
            "recommended_models": self.recommended_models,
            "setup_status": {
                "mistral_7b": "mistral:7b" in available_models,
                "llama2_7b": "llama2:7b" in available_models,
                "codellama_7b": "codellama:7b" in available_models,
                "phi3_3.8b": "phi3:3.8b" in available_models
            }
        }
        
        # Sauvegarder le rapport
        report_path = Path("ollama_setup_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Rapport sauvegard√©: {report_path}")
        return str(report_path)
    
    def run_setup(self) -> bool:
        """Ex√©cute la configuration compl√®te"""
        print("üéØ Configuration Ollama pour Semantic Pulse X")
        print("=" * 50)
        
        # V√©rifier si Ollama est disponible
        if not self.check_ollama_status():
            print("‚ùå Ollama n'est pas disponible!")
            print("üí° D√©marrez Ollama avec: docker-compose -f docker-compose.ollama.yml up -d")
            return False
        
        # Attendre qu'Ollama soit pr√™t
        if not self.wait_for_ollama():
            return False
        
        # Configurer les mod√®les
        results = self.setup_models()
        
        # G√©n√©rer le rapport
        report_path = self.generate_test_report()
        
        # Afficher le r√©sum√©
        print("\nüìà R√©sum√© de la configuration:")
        print(f"   - Mod√®les configur√©s: {sum(results.values())}/{len(results)}")
        print(f"   - Rapport: {report_path}")
        
        success_count = sum(results.values())
        if success_count > 0:
            print(f"‚úÖ Configuration r√©ussie! {success_count} mod√®les disponibles.")
            return True
        else:
            print("‚ùå Aucun mod√®le configur√© avec succ√®s.")
            return False


def main():
    """Fonction principale"""
    setup = OllamaSetup()
    
    if len(sys.argv) > 1 and sys.argv[1] == "docker":
        print("üê≥ Mode Docker d√©tect√©")
        # Attendre plus longtemps en mode Docker
        setup.wait_for_ollama(600)
    
    success = setup.run_setup()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
