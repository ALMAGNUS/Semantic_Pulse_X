#!/usr/bin/env python3
"""
D√©monstration Data Engineering - Semantic Pulse X
Script simple pour montrer les comp√©tences de nettoyage de donn√©es
"""

import json
import hashlib
from datetime import datetime
from pathlib import Path

def anonymize_user_id(user_id):
    """Anonymise un ID utilisateur avec SHA-256"""
    return hashlib.sha256(user_id.encode()).hexdigest()[:16]

def hash_text(text):
    """Hash un texte avec SHA-256"""
    return hashlib.sha256(text.encode()).hexdigest()

def demo_data_engineering():
    """D√©monstration compl√®te du data engineering"""
    print("üîç D√âMONSTRATION DATA ENGINEERING - Semantic Pulse X")
    print("=" * 70)
    print("üéØ Comp√©tences d√©montr√©es: Nettoyage, D√©doublonnage, Anonymisation RGPD")
    print("=" * 70)
    
    # 1. DONN√âES BRUTES
    print("\nüìä √âTAPE 1: DONN√âES BRUTES")
    print("-" * 50)
    
    raw_data = [
        {
            "id": 1,
            "texte": "J'ADORE cette √©mission !!! C'est G√âNIAL !!!",
            "auteur": "user123",
            "date": "2024-01-15 20:30:00",
            "source": "twitter",
            "email": "user123@email.com"
        },
        {
            "id": 2,
            "texte": "Cette √©mission est NULle, je d√©teste !!!",
            "auteur": "user456",
            "date": "15/01/2024 20:35",
            "source": "youtube",
            "email": "user456@gmail.com"
        },
        {
            "id": 3,
            "texte": "Super √©pisode, j'ai h√¢te de voir la suite !",
            "auteur": "user789",
            "date": "2024-01-15T20:40:00Z",
            "source": "instagram",
            "email": "user789@outlook.com"
        },
        {
            "id": 4,
            "texte": "J'ADORE cette √©mission !!! C'est G√âNIAL !!!",  # Doublon
            "auteur": "user999",
            "date": "2024-01-15 20:45:00",
            "source": "twitter",
            "email": "user999@email.com"
        }
    ]
    
    print(f"‚úÖ {len(raw_data)} donn√©es brutes collect√©es")
    print("üìã Exemples de donn√©es brutes:")
    for i, data in enumerate(raw_data, 1):
        print(f"   {i}. Texte: '{data['texte']}'")
        print(f"      Auteur: {data['auteur']} | Email: {data['email']}")
        print(f"      Date: {data['date']} | Source: {data['source']}")
        print()
    
    # 2. NETTOYAGE
    print("üßπ √âTAPE 2: NETTOYAGE DES DONN√âES")
    print("-" * 50)
    
    cleaned_data = []
    for data in raw_data:
        print(f"üîÑ Nettoyage de: '{data['texte']}'")
        
        # Nettoyage du texte
        texte_clean = data['texte']
        # Supprimer caract√®res sp√©ciaux r√©p√©t√©s
        texte_clean = texte_clean.replace('!!!', '').replace('!!', '').replace('!', '')
        # Normaliser la casse
        texte_clean = texte_clean.lower()
        # Supprimer espaces multiples
        texte_clean = ' '.join(texte_clean.split())
        
        print(f"   ‚Üí Suppression caract√®res sp√©ciaux: '{texte_clean}'")
        
        # Normalisation de la date
        date_str = data['date']
        try:
            if '/' in date_str:
                # Format fran√ßais
                date_obj = datetime.strptime(date_str, '%d/%m/%Y %H:%M')
            elif 'T' in date_str:
                # Format ISO
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            else:
                # Format standard
                date_obj = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
            
            date_normalized = date_obj.strftime('%Y-%m-%d %H:%M:%S')
        except:
            date_normalized = data['date']
        
        print(f"   ‚Üí Normalisation date: '{date_normalized}'")
        
        # Supprimer email (donn√©es sensibles)
        data_clean = {
            'id': data['id'],
            'texte': texte_clean,
            'auteur': data['auteur'],
            'date': date_normalized,
            'source': data['source'].lower()
        }
        
        cleaned_data.append(data_clean)
        print(f"   ‚úÖ Donn√©e nettoy√©e")
        print()
    
    print(f"‚úÖ {len(cleaned_data)} donn√©es nettoy√©es")
    
    # 3. D√âDOUBLONNAGE
    print("\nüîÑ √âTAPE 3: D√âDOUBLONNAGE")
    print("-" * 50)
    
    print(f"üìä Avant d√©doublonnage: {len(cleaned_data)} donn√©es")
    
    # D√©doublonnage par texte
    seen_texts = set()
    deduplicated_data = []
    doublons_removed = 0
    
    for data in cleaned_data:
        if data['texte'] not in seen_texts:
            seen_texts.add(data['texte'])
            deduplicated_data.append(data)
            print(f"   ‚úÖ Gard√©: '{data['texte']}' (ID: {data['id']})")
        else:
            doublons_removed += 1
            print(f"   ‚ùå Supprim√© (doublon): '{data['texte']}' (ID: {data['id']})")
    
    print(f"‚úÖ Apr√®s d√©doublonnage: {len(deduplicated_data)} donn√©es")
    print(f"üìä Doublons supprim√©s: {doublons_removed}")
    
    # 4. ANONYMIZATION RGPD
    print("\nüîí √âTAPE 4: ANONYMIZATION RGPD")
    print("-" * 50)
    
    anonymized_data = []
    for data in deduplicated_data:
        print(f"üîÑ Anonymisation de l'auteur: '{data['auteur']}'")
        
        # Anonymiser l'auteur
        auteur_anonyme = anonymize_user_id(data['auteur'])
        print(f"   ‚Üí Auteur anonymis√©: '{auteur_anonyme}'")
        
        # Hash du texte pour tra√ßabilit√©
        texte_hash = hash_text(data['texte'])
        print(f"   ‚Üí Hash du texte: '{texte_hash[:20]}...'")
        
        data_anonyme = {
            'id': data['id'],
            'contenu': data['texte'],
            'contenu_hash': texte_hash,
            'utilisateur_anonyme': auteur_anonyme,
            'timestamp': data['date'],
            'source_type': data['source']
        }
        
        anonymized_data.append(data_anonyme)
        print(f"   ‚úÖ Donn√©e anonymis√©e")
        print()
    
    print(f"‚úÖ {len(anonymized_data)} donn√©es anonymis√©es")
    
    # 5. HOMOG√âN√âISATION
    print("\nüìè √âTAPE 5: HOMOG√âN√âISATION")
    print("-" * 50)
    
    homogenized_data = []
    for data in anonymized_data:
        print(f"üîÑ Homog√©n√©isation de la donn√©e {data['id']}")
        
        # Calculer des m√©triques
        longueur = len(data['contenu'])
        mots = len(data['contenu'].split())
        
        # Ajouter des champs standardis√©s
        data_homogene = {
            'id': data['id'],
            'contenu': data['contenu'],
            'contenu_hash': data['contenu_hash'],
            'utilisateur_anonyme': data['utilisateur_anonyme'],
            'timestamp': data['timestamp'],
            'source_type': data['source_type'],
            'longueur_caracteres': longueur,
            'nombre_mots': mots,
            'densite_mots': round(mots / longueur * 100, 2) if longueur > 0 else 0
        }
        
        homogenized_data.append(data_homogene)
        print(f"   ‚Üí M√©triques: {mots} mots, {longueur} caract√®res, densit√© {data_homogene['densite_mots']}%")
        print(f"   ‚úÖ Donn√©e homog√©n√©is√©e")
        print()
    
    print(f"‚úÖ {len(homogenized_data)} donn√©es homog√©n√©is√©es")
    
    # 6. AGR√âGATION ET STATISTIQUES
    print("\nüìà √âTAPE 6: AGR√âGATION ET STATISTIQUES")
    print("-" * 50)
    
    # Statistiques par source
    sources_stats = {}
    for data in homogenized_data:
        source = data['source_type']
        if source not in sources_stats:
            sources_stats[source] = {'count': 0, 'total_mots': 0, 'total_caracteres': 0}
        
        sources_stats[source]['count'] += 1
        sources_stats[source]['total_mots'] += data['nombre_mots']
        sources_stats[source]['total_caracteres'] += data['longueur_caracteres']
    
    print("üìä R√©partition par source:")
    for source, stats in sources_stats.items():
        avg_mots = stats['total_mots'] / stats['count']
        avg_caracteres = stats['total_caracteres'] / stats['count']
        print(f"   - {source.upper()}: {stats['count']} donn√©es")
        print(f"     Moyenne: {avg_mots:.1f} mots, {avg_caracteres:.1f} caract√®res")
        print()
    
    # Statistiques globales
    total_mots = sum(data['nombre_mots'] for data in homogenized_data)
    total_caracteres = sum(data['longueur_caracteres'] for data in homogenized_data)
    longueur_moyenne = total_caracteres / len(homogenized_data)
    mots_moyens = total_mots / len(homogenized_data)
    
    print("üìà Statistiques globales:")
    print(f"   - Total donn√©es: {len(homogenized_data)}")
    print(f"   - Total mots: {total_mots}")
    print(f"   - Total caract√®res: {total_caracteres}")
    print(f"   - Moyenne mots par donn√©e: {mots_moyens:.1f}")
    print(f"   - Moyenne caract√®res par donn√©e: {longueur_moyenne:.1f}")
    
    # 7. SAUVEGARDE
    print("\nüíæ √âTAPE 7: SAUVEGARDE")
    print("-" * 50)
    
    # Cr√©er le dossier de sortie
    output_dir = Path("data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarder en JSON
    output_file = output_dir / "donnees_traitees_demo.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(homogenized_data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Donn√©es sauvegard√©es: {output_file}")
    print(f"   Taille: {output_file.stat().st_size} octets")
    
    # Cr√©er un rapport de qualit√©
    rapport = {
        "timestamp": datetime.now().isoformat(),
        "donnees_brutes": len(raw_data),
        "donnees_nettoyees": len(cleaned_data),
        "donnees_dedupliquees": len(deduplicated_data),
        "donnees_anonymisees": len(anonymized_data),
        "donnees_finales": len(homogenized_data),
        "doublons_supprimes": doublons_removed,
        "taux_deduplication": round(doublons_removed / len(raw_data) * 100, 2),
        "statistiques_globales": {
            "total_mots": total_mots,
            "total_caracteres": total_caracteres,
            "moyenne_mots": round(mots_moyens, 2),
            "moyenne_caracteres": round(longueur_moyenne, 2)
        },
        "sources": sources_stats
    }
    
    rapport_file = output_dir / "rapport_qualite_demo.json"
    with open(rapport_file, 'w', encoding='utf-8') as f:
        json.dump(rapport, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Rapport de qualit√©: {rapport_file}")
    
    # 8. R√âSUM√â FINAL
    print("\nüéâ R√âSUM√â FINAL - COMP√âTENCES D√âMONTR√âES")
    print("=" * 70)
    print("‚úÖ NETTOYAGE DES DONN√âES:")
    print("   - Suppression caract√®res sp√©ciaux")
    print("   - Normalisation casse")
    print("   - Standardisation formats de date")
    print("   - Suppression donn√©es sensibles (emails)")
    print()
    print("‚úÖ D√âDOUBLONNAGE:")
    print(f"   - {doublons_removed} doublons d√©tect√©s et supprim√©s")
    print(f"   - Taux de d√©doublonnage: {rapport['taux_deduplication']}%")
    print()
    print("‚úÖ ANONYMIZATION RGPD:")
    print("   - Hachage SHA-256 des identifiants utilisateurs")
    print("   - Pseudonymisation des donn√©es")
    print("   - Suppression des donn√©es personnelles")
    print()
    print("‚úÖ HOMOG√âN√âISATION:")
    print("   - Formats de donn√©es standardis√©s")
    print("   - M√©triques calcul√©es (mots, caract√®res, densit√©)")
    print("   - Structure de donn√©es uniforme")
    print()
    print("‚úÖ AGR√âGATION MULTI-SOURCES:")
    print("   - Groupement par source")
    print("   - Statistiques d√©taill√©es")
    print("   - M√©triques de qualit√©")
    print()
    print("‚úÖ CONFORMIT√â RGPD:")
    print("   - Aucune donn√©e personnelle conserv√©e")
    print("   - Tra√ßabilit√© via hachage")
    print("   - Anonymisation irr√©versible")
    print()
    print("üìä FICHIERS G√âN√âR√âS:")
    print(f"   - {output_file}")
    print(f"   - {rapport_file}")
    print()
    print("üéØ R√âSULTAT: Pipeline de data engineering complet et conforme RGPD !")

if __name__ == "__main__":
    demo_data_engineering()
