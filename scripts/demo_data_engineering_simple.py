#!/usr/bin/env python3
"""
DÃ©monstration Data Engineering - Semantic Pulse X
Script simple pour montrer les compÃ©tences de nettoyage de donnÃ©es
"""

import json
import hashlib
from datetime import datetime
from pathlib import Path

def anonymize_user_id(user_id):
    """Anonymise un ID utilisateur avec SHA-256"""
    return hashlib.sha256(user_id.encode()).hexdigest()[:16]

def hash_text(text):
    """Hash un texte avec SHA-256"""
    return hashlib.sha256(text.encode()).hexdigest()

def demo_data_engineering():
    """DÃ©monstration complÃ¨te du data engineering"""
    print("ðŸ” DÃ‰MONSTRATION DATA ENGINEERING - Semantic Pulse X")
    print("=" * 70)
    print("ðŸŽ¯ CompÃ©tences dÃ©montrÃ©es: Nettoyage, DÃ©doublonnage, Anonymisation RGPD")
    print("=" * 70)
    
    # 1. DONNÃ‰ES BRUTES
    print("\nðŸ“Š Ã‰TAPE 1: DONNÃ‰ES BRUTES")
    print("-" * 50)
    
    raw_data = [
        {
            "id": 1,
            "texte": "J'ADORE cette Ã©mission !!! C'est GÃ‰NIAL !!!",
            "auteur": "user123",
            "date": "2024-01-15 20:30:00",
            "source": "twitter",
            "email": "user123@email.com"
        },
        {
            "id": 2,
            "texte": "Cette Ã©mission est NULle, je dÃ©teste !!!",
            "auteur": "user456",
            "date": "15/01/2024 20:35",
            "source": "youtube",
            "email": "user456@gmail.com"
        },
        {
            "id": 3,
            "texte": "Super Ã©pisode, j'ai hÃ¢te de voir la suite !",
            "auteur": "user789",
            "date": "2024-01-15T20:40:00Z",
            "source": "instagram",
            "email": "user789@outlook.com"
        },
        {
            "id": 4,
            "texte": "J'ADORE cette Ã©mission !!! C'est GÃ‰NIAL !!!",  # Doublon
            "auteur": "user999",
            "date": "2024-01-15 20:45:00",
            "source": "twitter",
            "email": "user999@email.com"
        }
    ]
    
    print(f"âœ… {len(raw_data)} donnÃ©es brutes collectÃ©es")
    print("ðŸ“‹ Exemples de donnÃ©es brutes:")
    for i, data in enumerate(raw_data, 1):
        print(f"   {i}. Texte: '{data['texte']}'")
        print(f"      Auteur: {data['auteur']} | Email: {data['email']}")
        print(f"      Date: {data['date']} | Source: {data['source']}")
        print()
    
    # 2. NETTOYAGE
    print("ðŸ§¹ Ã‰TAPE 2: NETTOYAGE DES DONNÃ‰ES")
    print("-" * 50)
    
    cleaned_data = []
    for data in raw_data:
        print(f"ðŸ”„ Nettoyage de: '{data['texte']}'")
        
        # Nettoyage du texte
        texte_clean = data['texte']
        # Supprimer caractÃ¨res spÃ©ciaux rÃ©pÃ©tÃ©s
        texte_clean = texte_clean.replace('!!!', '').replace('!!', '').replace('!', '')
        # Normaliser la casse
        texte_clean = texte_clean.lower()
        # Supprimer espaces multiples
        texte_clean = ' '.join(texte_clean.split())
        
        print(f"   â†’ Suppression caractÃ¨res spÃ©ciaux: '{texte_clean}'")
        
        # Normalisation de la date
        date_str = data['date']
        try:
            if '/' in date_str:
                # Format franÃ§ais
                date_obj = datetime.strptime(date_str, '%d/%m/%Y %H:%M')
            elif 'T' in date_str:
                # Format ISO
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            else:
                # Format standard
                date_obj = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
            
            date_normalized = date_obj.strftime('%Y-%m-%d %H:%M:%S')
        except:
            date_normalized = data['date']
        
        print(f"   â†’ Normalisation date: '{date_normalized}'")
        
        # Supprimer email (donnÃ©es sensibles)
        data_clean = {
            'id': data['id'],
            'texte': texte_clean,
            'auteur': data['auteur'],
            'date': date_normalized,
            'source': data['source'].lower()
        }
        
        cleaned_data.append(data_clean)
        print(f"   âœ… DonnÃ©e nettoyÃ©e")
        print()
    
    print(f"âœ… {len(cleaned_data)} donnÃ©es nettoyÃ©es")
    
    # 3. DÃ‰DOUBLONNAGE
    print("\nðŸ”„ Ã‰TAPE 3: DÃ‰DOUBLONNAGE")
    print("-" * 50)
    
    print(f"ðŸ“Š Avant dÃ©doublonnage: {len(cleaned_data)} donnÃ©es")
    
    # DÃ©doublonnage par texte
    seen_texts = set()
    deduplicated_data = []
    doublons_removed = 0
    
    for data in cleaned_data:
        if data['texte'] not in seen_texts:
            seen_texts.add(data['texte'])
            deduplicated_data.append(data)
            print(f"   âœ… GardÃ©: '{data['texte']}' (ID: {data['id']})")
        else:
            doublons_removed += 1
            print(f"   âŒ SupprimÃ© (doublon): '{data['texte']}' (ID: {data['id']})")
    
    print(f"âœ… AprÃ¨s dÃ©doublonnage: {len(deduplicated_data)} donnÃ©es")
    print(f"ðŸ“Š Doublons supprimÃ©s: {doublons_removed}")
    
    # 4. ANONYMIZATION RGPD
    print("\nðŸ”’ Ã‰TAPE 4: ANONYMIZATION RGPD")
    print("-" * 50)
    
    anonymized_data = []
    for data in deduplicated_data:
        print(f"ðŸ”„ Anonymisation de l'auteur: '{data['auteur']}'")
        
        # Anonymiser l'auteur
        auteur_anonyme = anonymize_user_id(data['auteur'])
        print(f"   â†’ Auteur anonymisÃ©: '{auteur_anonyme}'")
        
        # Hash du texte pour traÃ§abilitÃ©
        texte_hash = hash_text(data['texte'])
        print(f"   â†’ Hash du texte: '{texte_hash[:20]}...'")
        
        data_anonyme = {
            'id': data['id'],
            'contenu': data['texte'],
            'contenu_hash': texte_hash,
            'utilisateur_anonyme': auteur_anonyme,
            'timestamp': data['date'],
            'source_type': data['source']
        }
        
        anonymized_data.append(data_anonyme)
        print(f"   âœ… DonnÃ©e anonymisÃ©e")
        print()
    
    print(f"âœ… {len(anonymized_data)} donnÃ©es anonymisÃ©es")
    
    # 5. HOMOGÃ‰NÃ‰ISATION
    print("\nðŸ“ Ã‰TAPE 5: HOMOGÃ‰NÃ‰ISATION")
    print("-" * 50)
    
    homogenized_data = []
    for data in anonymized_data:
        print(f"ðŸ”„ HomogÃ©nÃ©isation de la donnÃ©e {data['id']}")
        
        # Calculer des mÃ©triques
        longueur = len(data['contenu'])
        mots = len(data['contenu'].split())
        
        # Ajouter des champs standardisÃ©s
        data_homogene = {
            'id': data['id'],
            'contenu': data['contenu'],
            'contenu_hash': data['contenu_hash'],
            'utilisateur_anonyme': data['utilisateur_anonyme'],
            'timestamp': data['timestamp'],
            'source_type': data['source_type'],
            'longueur_caracteres': longueur,
            'nombre_mots': mots,
            'densite_mots': round(mots / longueur * 100, 2) if longueur > 0 else 0
        }
        
        homogenized_data.append(data_homogene)
        print(f"   â†’ MÃ©triques: {mots} mots, {longueur} caractÃ¨res, densitÃ© {data_homogene['densite_mots']}%")
        print(f"   âœ… DonnÃ©e homogÃ©nÃ©isÃ©e")
        print()
    
    print(f"âœ… {len(homogenized_data)} donnÃ©es homogÃ©nÃ©isÃ©es")
    
    # 6. AGRÃ‰GATION ET STATISTIQUES
    print("\nðŸ“ˆ Ã‰TAPE 6: AGRÃ‰GATION ET STATISTIQUES")
    print("-" * 50)
    
    # Statistiques par source
    sources_stats = {}
    for data in homogenized_data:
        source = data['source_type']
        if source not in sources_stats:
            sources_stats[source] = {'count': 0, 'total_mots': 0, 'total_caracteres': 0}
        
        sources_stats[source]['count'] += 1
        sources_stats[source]['total_mots'] += data['nombre_mots']
        sources_stats[source]['total_caracteres'] += data['longueur_caracteres']
    
    print("ðŸ“Š RÃ©partition par source:")
    for source, stats in sources_stats.items():
        avg_mots = stats['total_mots'] / stats['count']
        avg_caracteres = stats['total_caracteres'] / stats['count']
        print(f"   - {source.upper()}: {stats['count']} donnÃ©es")
        print(f"     Moyenne: {avg_mots:.1f} mots, {avg_caracteres:.1f} caractÃ¨res")
        print()
    
    # Statistiques globales
    total_mots = sum(data['nombre_mots'] for data in homogenized_data)
    total_caracteres = sum(data['longueur_caracteres'] for data in homogenized_data)
    longueur_moyenne = total_caracteres / len(homogenized_data)
    mots_moyens = total_mots / len(homogenized_data)
    
    print("ðŸ“ˆ Statistiques globales:")
    print(f"   - Total donnÃ©es: {len(homogenized_data)}")
    print(f"   - Total mots: {total_mots}")
    print(f"   - Total caractÃ¨res: {total_caracteres}")
    print(f"   - Moyenne mots par donnÃ©e: {mots_moyens:.1f}")
    print(f"   - Moyenne caractÃ¨res par donnÃ©e: {longueur_moyenne:.1f}")
    
    # 7. SAUVEGARDE
    print("\nðŸ’¾ Ã‰TAPE 7: SAUVEGARDE")
    print("-" * 50)
    
    # CrÃ©er le dossier de sortie
    output_dir = Path("data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarder en JSON
    output_file = output_dir / "donnees_traitees_demo.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(homogenized_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… DonnÃ©es sauvegardÃ©es: {output_file}")
    print(f"   Taille: {output_file.stat().st_size} octets")
    
    # CrÃ©er un rapport de qualitÃ©
    rapport = {
        "timestamp": datetime.now().isoformat(),
        "donnees_brutes": len(raw_data),
        "donnees_nettoyees": len(cleaned_data),
        "donnees_dedupliquees": len(deduplicated_data),
        "donnees_anonymisees": len(anonymized_data),
        "donnees_finales": len(homogenized_data),
        "doublons_supprimes": doublons_removed,
        "taux_deduplication": round(doublons_removed / len(raw_data) * 100, 2),
        "statistiques_globales": {
            "total_mots": total_mots,
            "total_caracteres": total_caracteres,
            "moyenne_mots": round(mots_moyens, 2),
            "moyenne_caracteres": round(longueur_moyenne, 2)
        },
        "sources": sources_stats
    }
    
    rapport_file = output_dir / "rapport_qualite_demo.json"
    with open(rapport_file, 'w', encoding='utf-8') as f:
        json.dump(rapport, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Rapport de qualitÃ©: {rapport_file}")
    
    # 8. RÃ‰SUMÃ‰ FINAL
    print("\nðŸŽ‰ RÃ‰SUMÃ‰ FINAL - COMPÃ‰TENCES DÃ‰MONTRÃ‰ES")
    print("=" * 70)
    print("âœ… NETTOYAGE DES DONNÃ‰ES:")
    print("   - Suppression caractÃ¨res spÃ©ciaux")
    print("   - Normalisation casse")
    print("   - Standardisation formats de date")
    print("   - Suppression donnÃ©es sensibles (emails)")
    print()
    print("âœ… DÃ‰DOUBLONNAGE:")
    print(f"   - {doublons_removed} doublons dÃ©tectÃ©s et supprimÃ©s")
    print(f"   - Taux de dÃ©doublonnage: {rapport['taux_deduplication']}%")
    print()
    print("âœ… ANONYMIZATION RGPD:")
    print("   - Hachage SHA-256 des identifiants utilisateurs")
    print("   - Pseudonymisation des donnÃ©es")
    print("   - Suppression des donnÃ©es personnelles")
    print()
    print("âœ… HOMOGÃ‰NÃ‰ISATION:")
    print("   - Formats de donnÃ©es standardisÃ©s")
    print("   - MÃ©triques calculÃ©es (mots, caractÃ¨res, densitÃ©)")
    print("   - Structure de donnÃ©es uniforme")
    print()
    print("âœ… AGRÃ‰GATION MULTI-SOURCES:")
    print("   - Groupement par source")
    print("   - Statistiques dÃ©taillÃ©es")
    print("   - MÃ©triques de qualitÃ©")
    print()
    print("âœ… CONFORMITÃ‰ RGPD:")
    print("   - Aucune donnÃ©e personnelle conservÃ©e")
    print("   - TraÃ§abilitÃ© via hachage")
    print("   - Anonymisation irrÃ©versible")
    print()
    print("ðŸ“Š FICHIERS GÃ‰NÃ‰RÃ‰S:")
    print(f"   - {output_file}")
    print(f"   - {rapport_file}")
    print()
    print("ðŸŽ¯ RÃ‰SULTAT: Pipeline de data engineering complet et conforme RGPD !")

if __name__ == "__main__":
    demo_data_engineering()
