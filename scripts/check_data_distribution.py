#!/usr/bin/env python3
"""
VÃ©rificateur et correcteur de rÃ©partition des donnÃ©es - Semantic Pulse X
VÃ©rifie la rÃ©partition 1/3 CSV, 1/3 Base relationnelle, 1/3 Big Data
"""

import sqlite3
import pandas as pd
import logging
from pathlib import Path

# Configuration du logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_data_distribution():
    """VÃ©rifie la rÃ©partition actuelle des donnÃ©es."""
    
    logger.info("ğŸ” VÃ‰RIFICATION DE LA RÃ‰PARTITION DES DONNÃ‰ES")
    logger.info("=" * 60)
    
    # 1. VÃ©rifier la base de donnÃ©es
    logger.info("ğŸ—„ï¸ VÃ‰RIFICATION BASE DE DONNÃ‰ES:")
    db_file = Path("semantic_pulse.db")
    
    if db_file.exists():
        try:
            conn = sqlite3.connect(str(db_file))
            cursor = conn.cursor()
            
            # Lister les tables
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            if tables:
                logger.info(f"  âœ… Tables trouvÃ©es: {len(tables)}")
                for table_name, in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                    count = cursor.fetchone()[0]
                    logger.info(f"    ğŸ“Š {table_name}: {count} enregistrements")
            else:
                logger.info("  âŒ Aucune table dans la base")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"  âŒ Erreur base de donnÃ©es: {e}")
    else:
        logger.info("  âŒ Fichier semantic_pulse.db non trouvÃ©")
    
    # 2. VÃ©rifier les fichiers CSV
    logger.info("\nğŸ“„ VÃ‰RIFICATION FICHIERS CSV:")
    csv_dir = Path("data/raw/kaggle_tweets")
    
    if csv_dir.exists():
        csv_files = list(csv_dir.glob("*.csv"))
        total_csv_rows = 0
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file, nrows=5)  # Juste pour vÃ©rifier
                full_df = pd.read_csv(csv_file)
                total_csv_rows += len(full_df)
                logger.info(f"  âœ… {csv_file.name}: {len(full_df)} lignes")
            except Exception as e:
                logger.warning(f"  âš ï¸ Erreur lecture {csv_file.name}: {e}")
        
        logger.info(f"  ğŸ“Š Total CSV: {total_csv_rows} lignes")
    else:
        logger.info("  âŒ Dossier CSV non trouvÃ©")
    
    # 3. VÃ©rifier les fichiers Parquet (Big Data)
    logger.info("\nğŸ“Š VÃ‰RIFICATION BIG DATA (PARQUET):")
    parquet_dir = Path("data/processed/bigdata")
    
    if parquet_dir.exists():
        parquet_files = list(parquet_dir.glob("*.parquet"))
        total_parquet_rows = 0
        
        for parquet_file in parquet_files:
            try:
                df = pd.read_parquet(parquet_file)
                total_parquet_rows += len(df)
                logger.info(f"  âœ… {parquet_file.name}: {len(df)} lignes")
            except Exception as e:
                logger.warning(f"  âš ï¸ Erreur lecture {parquet_file.name}: {e}")
        
        logger.info(f"  ğŸ“Š Total Parquet: {total_parquet_rows} lignes")
    else:
        logger.info("  âŒ Dossier Parquet non trouvÃ©")
    
    # 4. Analyser la rÃ©partition
    logger.info("\nğŸ“Š ANALYSE DE LA RÃ‰PARTITION:")
    
    # Compter le total des donnÃ©es Kaggle originales
    original_file = Path("data/raw/kaggle_tweets/sentiment140.csv")
    if original_file.exists():
        try:
            df_original = pd.read_csv(original_file)
            total_original = len(df_original)
            logger.info(f"  ğŸ“ˆ Dataset original: {total_original} lignes")
            
            # Calculer les tiers attendus
            tier_size = total_original // 3
            logger.info(f"  ğŸ¯ Taille attendue par tiers: {tier_size} lignes")
            
            # VÃ©rifier la rÃ©partition actuelle
            logger.info(f"  ğŸ“„ CSV actuel: {total_csv_rows} lignes")
            logger.info(f"  ğŸ—„ï¸ Base relationnelle: 0 lignes (Ã  vÃ©rifier)")
            logger.info(f"  ğŸ“Š Parquet actuel: {total_parquet_rows} lignes")
            
            # Diagnostiquer le problÃ¨me
            logger.info("\nğŸ” DIAGNOSTIC:")
            if total_parquet_rows > tier_size * 2:
                logger.warning("  âš ï¸ PROBLÃˆME: Trop de donnÃ©es en Parquet")
                logger.info("  ğŸ’¡ Solution: RÃ©partir Ã©quitablement en 3 tiers")
            elif total_csv_rows > tier_size:
                logger.warning("  âš ï¸ PROBLÃˆME: Trop de donnÃ©es en CSV")
            else:
                logger.info("  âœ… RÃ©partition CSV/Parquet correcte")
            
            logger.info("  âŒ PROBLÃˆME PRINCIPAL: Base relationnelle vide")
            logger.info("  ğŸ’¡ Solution: CrÃ©er les tables Merise et insÃ©rer 1/3 des donnÃ©es")
            
        except Exception as e:
            logger.error(f"  âŒ Erreur lecture fichier original: {e}")
    else:
        logger.info("  âŒ Fichier original sentiment140.csv non trouvÃ©")
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ¯ RECOMMANDATIONS:")
    logger.info("=" * 60)
    logger.info("1. ğŸ—ï¸ CrÃ©er les tables Merise (programmes, diffusions, reactions, etc.)")
    logger.info("2. ğŸ“Š Diviser le dataset en 3 tiers Ã©gaux")
    logger.info("3. ğŸ—„ï¸ InsÃ©rer 1/3 dans la base relationnelle")
    logger.info("4. ğŸ“„ Garder 1/3 en CSV")
    logger.info("5. ğŸ“Š Garder 1/3 en Parquet")
    logger.info("6. ğŸ”— Tester les relations cardinales")
    logger.info("=" * 60)

def main():
    """Fonction principale."""
    logger.info("ğŸ” VÃ‰RIFICATEUR DE RÃ‰PARTITION DES DONNÃ‰ES")
    
    check_data_distribution()
    
    return True

if __name__ == "__main__":
    main()




