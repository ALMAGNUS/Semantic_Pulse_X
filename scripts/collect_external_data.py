#!/usr/bin/env python3
"""
Collecte de donn√©es Phase 2 - Semantic Pulse X
Approche simple et progressive
"""

import pandas as pd
import requests
import logging
from pathlib import Path
from datetime import datetime
import time
import json

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_news_data():
    """Collecte de donn√©es d'actualit√©s simples"""
    logger.info("üì∞ Collecte de donn√©es d'actualit√©s")
    
    # Sources d'actualit√©s gratuites
    news_sources = [
        "https://httpbin.org/json",  # API de test
        "https://jsonplaceholder.typicode.com/posts"  # Donn√©es de test
    ]
    
    collected_data = []
    
    for source in news_sources:
        try:
            logger.info(f"üîç Collecte depuis: {source}")
            response = requests.get(source, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"‚úÖ {len(data) if isinstance(data, list) else 1} √©l√©ments collect√©s")
                
                # Formatage des donn√©es
                if isinstance(data, list):
                    for item in data[:5]:  # Limite √† 5 √©l√©ments
                        collected_data.append({
                            'source': 'news_api',
                            'title': item.get('title', 'Titre par d√©faut'),
                            'content': item.get('body', 'Contenu par d√©faut'),
                            'date': datetime.now().isoformat(),
                            'url': source
                        })
                else:
                    collected_data.append({
                        'source': 'news_api',
                        'title': data.get('title', 'Titre par d√©faut'),
                        'content': str(data),
                        'date': datetime.now().isoformat(),
                        'url': source
                    })
                
                time.sleep(1)  # D√©lai entre les requ√™tes
                
            else:
                logger.warning(f"‚ö†Ô∏è Erreur {response.status_code} pour {source}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte {source}: {e}")
    
    logger.info(f"üìä Total collect√©: {len(collected_data)} articles")
    return collected_data

def collect_social_data():
    """Collecte de donn√©es sociales simul√©es"""
    logger.info("üì± Collecte de donn√©es sociales")
    
    # Donn√©es simul√©es pour d√©monstration
    social_data = [
        {
            'source': 'youtube_simulated',
            'video_title': 'Actualit√©s du jour - √âmotions m√©diatiques',
            'comment': 'Tr√®s int√©ressant cette analyse des √©motions !',
            'date': datetime.now().isoformat(),
            'platform': 'youtube'
        },
        {
            'source': 'instagram_simulated', 
            'post_text': 'Nouvelle tendance √©motionnelle d√©tect√©e üìä',
            'comment': 'Super analyse !',
            'date': datetime.now().isoformat(),
            'platform': 'instagram'
        },
        {
            'source': 'twitter_simulated',
            'tweet': 'Les √©motions dans les m√©dias √©voluent rapidement',
            'date': datetime.now().isoformat(),
            'platform': 'twitter'
        }
    ]
    
    logger.info(f"üìä Total collect√©: {len(social_data)} posts sociaux")
    return social_data

def collect_web_scraping_data():
    """Collecte de donn√©es par web scraping simple"""
    logger.info("üï∑Ô∏è Collecte par web scraping")
    
    # Scraping simple d'une page de test
    try:
        from bs4 import BeautifulSoup
        
        # Page de test simple
        test_url = "https://httpbin.org/html"
        response = requests.get(test_url, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extraction des donn√©es
            scraped_data = []
            paragraphs = soup.find_all('p')
            
            for i, p in enumerate(paragraphs[:3]):  # Limite √† 3 paragraphes
                scraped_data.append({
                    'source': 'web_scraping',
                    'content': p.get_text().strip(),
                    'date': datetime.now().isoformat(),
                    'url': test_url,
                    'paragraph_id': i
                })
            
            logger.info(f"üìä Total scrap√©: {len(scraped_data)} paragraphes")
            return scraped_data
            
        else:
            logger.warning(f"‚ö†Ô∏è Erreur scraping: {response.status_code}")
            return []
            
    except ImportError:
        logger.warning("‚ö†Ô∏è BeautifulSoup non install√©, utilisation de donn√©es simul√©es")
        return [{
            'source': 'web_scraping',
            'content': 'Contenu simul√© pour d√©monstration',
            'date': datetime.now().isoformat(),
            'url': 'simulated'
        }]
    except Exception as e:
        logger.error(f"‚ùå Erreur scraping: {e}")
        return []

def save_collected_data(all_data):
    """Sauvegarde des donn√©es collect√©es"""
    logger.info("üíæ Sauvegarde des donn√©es collect√©es")
    
    # Cr√©ation du dossier
    output_dir = Path("data/raw/external_apis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarde en JSON
    json_file = output_dir / f"external_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ Donn√©es sauvegard√©es: {json_file}")
    
    # Sauvegarde en CSV
    csv_file = output_dir / f"external_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df = pd.DataFrame(all_data)
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    logger.info(f"‚úÖ Donn√©es CSV: {csv_file}")
    
    return json_file, csv_file

def main():
    """Fonction principale de collecte Phase 2"""
    logger.info("üöÄ D√âMARRAGE COLLECTE PHASE 2 - APIs EXTERNES")
    logger.info("=" * 60)
    
    all_data = []
    
    # 1. Collecte d'actualit√©s
    news_data = collect_news_data()
    all_data.extend(news_data)
    
    # 2. Collecte de donn√©es sociales
    social_data = collect_social_data()
    all_data.extend(social_data)
    
    # 3. Collecte par web scraping
    scraping_data = collect_web_scraping_data()
    all_data.extend(scraping_data)
    
    # 4. Sauvegarde
    if all_data:
        json_file, csv_file = save_collected_data(all_data)
        
        # R√©sum√©
        logger.info("\n" + "=" * 60)
        logger.info("üìä R√âSUM√â COLLECTE PHASE 2")
        logger.info("=" * 60)
        logger.info(f"üìà Total donn√©es collect√©es: {len(all_data)}")
        
        # Par source
        sources = {}
        for item in all_data:
            source = item.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1
        
        logger.info("üìã R√©partition par source:")
        for source, count in sources.items():
            logger.info(f"   {source}: {count} √©l√©ments")
        
        logger.info(f"üíæ Fichiers cr√©√©s:")
        logger.info(f"   JSON: {json_file}")
        logger.info(f"   CSV: {csv_file}")
        
        logger.info("‚úÖ PHASE 2 COLLECTE TERMIN√âE!")
        return True
    else:
        logger.error("‚ùå Aucune donn√©e collect√©e")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)




