#!/usr/bin/env python3
"""
Visualisation des rÃ©sultats du data engineering - Semantic Pulse X
Montre toutes les Ã©tapes du traitement des donnÃ©es avec des dÃ©tails concrets
"""

import os
import json
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path

# Configuration du logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def visualiser_etapes_data_engineering():
    """Visualise toutes les Ã©tapes du data engineering avec des dÃ©tails concrets."""
    logger.info("ğŸ” VISUALISATION COMPLÃˆTE DU DATA ENGINEERING")
    logger.info("=" * 80)
    
    # Ã‰tape 1: Sources de donnÃ©es
    logger.info("\nğŸ“Š Ã‰TAPE 1: SOURCES DE DONNÃ‰ES")
    logger.info("-" * 50)
    
    sources_dir = "data/raw"
    if os.path.exists(sources_dir):
        for root, dirs, files in os.walk(sources_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                logger.info(f"ğŸ“„ {file_path}")
                logger.info(f"   ğŸ’¾ Taille: {file_size:.3f} MB")
                
                # Analyser le contenu si c'est un CSV
                if file.endswith('.csv'):
                    try:
                        df = pd.read_csv(file_path, nrows=5)  # Lire seulement les 5 premiÃ¨res lignes
                        logger.info(f"   ğŸ“Š Colonnes: {list(df.columns)}")
                        logger.info(f"   ğŸ“ˆ Ã‰chantillon de donnÃ©es:")
                        for i, row in df.iterrows():
                            logger.info(f"      Ligne {i+1}: {dict(row)}")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ Impossible de lire le CSV: {e}")
    
    # Ã‰tape 2: DonnÃ©es traitÃ©es
    logger.info("\nğŸ”„ Ã‰TAPE 2: DONNÃ‰ES TRAITÃ‰ES")
    logger.info("-" * 50)
    
    processed_dir = "data/processed"
    if os.path.exists(processed_dir):
        for root, dirs, files in os.walk(processed_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                logger.info(f"ğŸ“„ {file_path}")
                logger.info(f"   ğŸ’¾ Taille: {file_size:.3f} MB")
                
                # Analyser le contenu selon le type
                if file.endswith('.parquet'):
                    try:
                        df = pd.read_parquet(file_path)
                        logger.info(f"   ğŸ“Š Lignes: {len(df):,}")
                        logger.info(f"   ğŸ“‹ Colonnes: {len(df.columns)}")
                        logger.info(f"   ğŸ·ï¸ Colonnes: {list(df.columns)}")
                        
                        # Statistiques de base
                        logger.info(f"   ğŸ“ˆ Statistiques:")
                        for col in df.columns:
                            if df[col].dtype in ['int64', 'float64']:
                                logger.info(f"      {col}: min={df[col].min()}, max={df[col].max()}, moyenne={df[col].mean():.2f}")
                            else:
                                unique_count = df[col].nunique()
                                logger.info(f"      {col}: {unique_count} valeurs uniques")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ Impossible de lire le Parquet: {e}")
                
                elif file.endswith('.json'):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        logger.info(f"   ğŸ“Š Type: {type(data).__name__}")
                        if isinstance(data, list):
                            logger.info(f"   ğŸ“ˆ Ã‰lÃ©ments: {len(data)}")
                            if data:
                                logger.info(f"   ğŸ·ï¸ ClÃ©s du premier Ã©lÃ©ment: {list(data[0].keys()) if isinstance(data[0], dict) else 'N/A'}")
                        elif isinstance(data, dict):
                            logger.info(f"   ğŸ·ï¸ ClÃ©s: {list(data.keys())}")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ Impossible de lire le JSON: {e}")
    
    # Ã‰tape 3: Analyse de qualitÃ©
    logger.info("\nğŸ” Ã‰TAPE 3: ANALYSE DE QUALITÃ‰")
    logger.info("-" * 50)
    
    # Analyser les fichiers Parquet
    parquet_files = []
    for root, dirs, files in os.walk(processed_dir):
        for file in files:
            if file.endswith('.parquet'):
                parquet_files.append(os.path.join(root, file))
    
    total_rows = 0
    total_size = 0
    
    for parquet_file in parquet_files:
        try:
            df = pd.read_parquet(parquet_file)
            file_size = os.path.getsize(parquet_file) / (1024 * 1024)
            
            logger.info(f"ğŸ“Š {os.path.basename(parquet_file)}:")
            logger.info(f"   ğŸ“ˆ Lignes: {len(df):,}")
            logger.info(f"   ğŸ’¾ Taille: {file_size:.3f} MB")
            
            # QualitÃ© des donnÃ©es
            missing_data = df.isnull().sum()
            if missing_data.sum() > 0:
                logger.info(f"   âš ï¸ DonnÃ©es manquantes:")
                for col, missing in missing_data.items():
                    if missing > 0:
                        logger.info(f"      {col}: {missing} ({missing/len(df)*100:.1f}%)")
            else:
                logger.info(f"   âœ… Aucune donnÃ©e manquante")
            
            # Types de donnÃ©es
            logger.info(f"   ğŸ·ï¸ Types de donnÃ©es:")
            for col, dtype in df.dtypes.items():
                logger.info(f"      {col}: {dtype}")
            
            total_rows += len(df)
            total_size += file_size
            
        except Exception as e:
            logger.error(f"âŒ Erreur analyse {parquet_file}: {e}")
    
    logger.info(f"\nğŸ“Š RÃ‰SUMÃ‰ GLOBAL:")
    logger.info(f"   ğŸ“ˆ Total lignes: {total_rows:,}")
    logger.info(f"   ğŸ’¾ Taille totale: {total_size:.3f} MB")
    
    # Ã‰tape 4: MÃ©triques de performance
    logger.info("\nâš¡ Ã‰TAPE 4: MÃ‰TRIQUES DE PERFORMANCE")
    logger.info("-" * 50)
    
    # Calculer la compression
    raw_size = 0
    for root, dirs, files in os.walk(sources_dir):
        for file in files:
            if file.endswith('.csv'):
                raw_size += os.path.getsize(os.path.join(root, file)) / (1024 * 1024)
    
    if raw_size > 0:
        compression_ratio = (1 - (total_size / raw_size)) * 100
        logger.info(f"ğŸ—œï¸ Compression CSV â†’ Parquet: {compression_ratio:.1f}%")
        logger.info(f"ğŸ“Š Taille originale CSV: {raw_size:.3f} MB")
        logger.info(f"ğŸ“Š Taille compressÃ©e Parquet: {total_size:.3f} MB")
        logger.info(f"ğŸ’° Ã‰conomie d'espace: {raw_size - total_size:.3f} MB")
    
    # Ã‰tape 5: TraÃ§abilitÃ©
    logger.info("\nğŸ” Ã‰TAPE 5: TRAÃ‡ABILITÃ‰")
    logger.info("-" * 50)
    
    # VÃ©rifier les logs et rapports
    log_files = []
    for root, dirs, files in os.walk(processed_dir):
        for file in files:
            if 'report' in file.lower() or 'log' in file.lower():
                log_files.append(os.path.join(root, file))
    
    for log_file in log_files:
        logger.info(f"ğŸ“‹ {os.path.basename(log_file)}")
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"   ğŸ“„ Taille: {len(content)} caractÃ¨res")
            logger.info(f"   ğŸ“… DerniÃ¨re modification: {datetime.fromtimestamp(os.path.getmtime(log_file))}")
        except Exception as e:
            logger.warning(f"   âš ï¸ Impossible de lire: {e}")
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… VISUALISATION COMPLÃˆTE TERMINÃ‰E")
    logger.info("=" * 80)

def analyser_pipeline_etl():
    """Analyse le pipeline ETL en dÃ©tail."""
    logger.info("\nğŸ”„ ANALYSE DÃ‰TAILLÃ‰E DU PIPELINE ETL")
    logger.info("=" * 60)
    
    # VÃ©rifier les Ã©tapes du pipeline
    pipeline_steps = [
        ("Extraction", "data/raw"),
        ("Transformation", "data/processed"),
        ("Chargement", "data/processed/bigdata")
    ]
    
    for step_name, step_dir in pipeline_steps:
        logger.info(f"\nğŸ“Š {step_name.upper()}:")
        logger.info("-" * 30)
        
        if os.path.exists(step_dir):
            files = []
            total_size = 0
            
            for root, dirs, filenames in os.walk(step_dir):
                for filename in filenames:
                    file_path = os.path.join(root, filename)
                    file_size = os.path.getsize(file_path) / (1024 * 1024)
                    files.append((filename, file_size))
                    total_size += file_size
            
            logger.info(f"   ğŸ“ Fichiers: {len(files)}")
            logger.info(f"   ğŸ’¾ Taille totale: {total_size:.3f} MB")
            
            for filename, size in files:
                logger.info(f"      ğŸ“„ {filename}: {size:.3f} MB")
        else:
            logger.warning(f"   âš ï¸ RÃ©pertoire {step_dir} non trouvÃ©")

def generer_rapport_complet():
    """GÃ©nÃ¨re un rapport complet du data engineering."""
    logger.info("\nğŸ“‹ GÃ‰NÃ‰RATION DU RAPPORT COMPLET")
    logger.info("=" * 50)
    
    rapport = {
        "timestamp": datetime.now().isoformat(),
        "data_engineering_summary": {
            "sources": [],
            "processed_files": [],
            "quality_metrics": {},
            "performance_metrics": {}
        }
    }
    
    # Analyser les sources
    sources_dir = "data/raw"
    if os.path.exists(sources_dir):
        for root, dirs, files in os.walk(sources_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path) / (1024 * 1024)
                rapport["data_engineering_summary"]["sources"].append({
                    "file": file_path,
                    "size_mb": round(file_size, 3)
                })
    
    # Analyser les fichiers traitÃ©s
    processed_dir = "data/processed"
    if os.path.exists(processed_dir):
        for root, dirs, files in os.walk(processed_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path) / (1024 * 1024)
                rapport["data_engineering_summary"]["processed_files"].append({
                    "file": file_path,
                    "size_mb": round(file_size, 3)
                })
    
    # Sauvegarder le rapport
    rapport_file = "data/processed/rapport_data_engineering_complet.json"
    os.makedirs(os.path.dirname(rapport_file), exist_ok=True)
    
    with open(rapport_file, 'w', encoding='utf-8') as f:
        json.dump(rapport, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ Rapport sauvegardÃ©: {rapport_file}")
    return rapport

def main():
    """Fonction principale."""
    logger.info("ğŸ” VISUALISATION DES RÃ‰SULTATS - DATA ENGINEERING")
    logger.info("=" * 80)
    logger.info("ğŸ“Š Analyse complÃ¨te de toutes les Ã©tapes du traitement des donnÃ©es")
    logger.info("ğŸ” DÃ©tails concrets pour dÃ©monstration")
    logger.info("=" * 80)
    
    # Visualiser toutes les Ã©tapes
    visualiser_etapes_data_engineering()
    
    # Analyser le pipeline ETL
    analyser_pipeline_etl()
    
    # GÃ©nÃ©rer le rapport complet
    rapport = generer_rapport_complet()
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ VISUALISATION TERMINÃ‰E")
    logger.info("=" * 80)
    logger.info("ğŸ“Š Toutes les Ã©tapes du data engineering ont Ã©tÃ© analysÃ©es")
    logger.info("ğŸ” DÃ©tails concrets disponibles pour dÃ©monstration")
    logger.info("ğŸ“‹ Rapport complet gÃ©nÃ©rÃ©")
    logger.info("=" * 80)

if __name__ == "__main__":
    main()
