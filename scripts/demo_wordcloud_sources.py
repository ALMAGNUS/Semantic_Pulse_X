#!/usr/bin/env python3
"""
D√©monstration nuage de mots avec tra√ßabilit√© des sources - Semantic Pulse X
Montre l'origine de chaque mot (Twitter, YouTube, Instagram, etc.)
"""

import sys
from pathlib import Path
import json
import base64
from io import BytesIO

# Ajouter le r√©pertoire racine au PYTHONPATH
project_root = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(project_root))

def demo_wordcloud_with_sources():
    """D√©monstration du nuage de mots avec tra√ßabilit√© des sources"""
    print("üé® D√âMONSTRATION NUAGE DE MOTS AVEC SOURCES")
    print("=" * 60)
    
    try:
        from app.frontend.visualization.wordcloud_generator import wordcloud_generator
        
        # Charger les donn√©es trait√©es
        data_file = Path("data/processed/donnees_traitees_demo.json")
        if not data_file.exists():
            print("‚ùå Aucune donn√©e trait√©e trouv√©e")
            return
        
        with open(data_file, 'r', encoding='utf-8') as f:
            donnees = json.load(f)
        
        print(f"‚úÖ {len(donnees)} donn√©es charg√©es")
        
        # Pr√©parer les donn√©es avec sources
        data_with_sources = []
        for data in donnees:
            data_with_sources.append({
                'text': data['contenu'],
                'source': data['source_type'],
                'emotion': 'joie',  # Simuler une √©motion pour la d√©mo
                'user_id': data['utilisateur_anonyme']
            })
        
        print("\nüìä DONN√âES AVEC SOURCES:")
        for i, data in enumerate(data_with_sources, 1):
            print(f"   {i}. Texte: '{data['text']}'")
            print(f"      Source: {data['source'].upper()}")
            print(f"      Utilisateur: {data['user_id']}")
            print()
        
        print("üîÑ G√©n√©ration du nuage de mots avec tra√ßabilit√©...")
        
        # G√©n√©rer le nuage de mots avec sources
        wordcloud_data = wordcloud_generator.generate_emotion_wordcloud_with_sources(
            data=data_with_sources,
            emotion_filter="joie",
            max_words=15,
            width=800,
            height=400
        )
        
        if wordcloud_data and wordcloud_data.get('image'):
            print("‚úÖ Nuage de mots g√©n√©r√© avec succ√®s !")
            
            # Afficher les statistiques
            print(f"\nüìä STATISTIQUES:")
            print(f"   ‚Ä¢ Total mots: {wordcloud_data['total_words']}")
            print(f"   ‚Ä¢ Mots uniques: {wordcloud_data['unique_words']}")
            print(f"   ‚Ä¢ √âmotion: {wordcloud_data['emotion']}")
            
            # Afficher la tra√ßabilit√© des sources
            print(f"\nüîç TRACABILIT√â DES SOURCES:")
            print("=" * 50)
            
            source_traceability = wordcloud_data.get('source_traceability', {})
            for word, info in source_traceability.items():
                print(f"\nüìù Mot: '{word}' (fr√©quence: {info['frequency']})")
                print(f"   üéØ Source principale: {info['main_source'].upper()}")
                print(f"   üìä R√©partition par source:")
                for source, count in info['sources'].items():
                    percentage = (count / info['frequency']) * 100
                    print(f"      ‚Ä¢ {source.upper()}: {count} fois ({percentage:.1f}%)")
            
            # Sauvegarder l'image
            output_file = Path("data/processed/wordcloud_sources_demo.png")
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # D√©coder l'image base64
            image_data = base64.b64decode(wordcloud_data['image'])
            
            with open(output_file, 'wb') as f:
                f.write(image_data)
            
            print(f"\nüíæ Nuage de mots sauvegard√©: {output_file}")
            print(f"   Taille: {len(image_data)} octets")
            
            # G√©n√©rer un rapport d√©taill√©
            rapport = {
                "nuage_mots_sources": {
                    "fichier": str(output_file),
                    "emotion": wordcloud_data['emotion'],
                    "total_mots": wordcloud_data['total_words'],
                    "mots_uniques": wordcloud_data['unique_words'],
                    "tra√ßabilite_sources": source_traceability
                },
                "donnees_source": {
                    "nombre_donnees": len(donnees),
                    "sources_detectees": list(set(data['source_type'] for data in donnees)),
                    "repartition_sources": {
                        source: len([d for d in donnees if d['source_type'] == source])
                        for source in set(data['source_type'] for data in donnees)
                    }
                },
                "generation": {
                    "timestamp": wordcloud_data['metadata']['generated_at'],
                    "parametres": {
                        "max_words": 15,
                        "width": 800,
                        "height": 400
                    }
                }
            }
            
            rapport_file = Path("data/processed/rapport_wordcloud_sources_demo.json")
            with open(rapport_file, 'w', encoding='utf-8') as f:
                json.dump(rapport, f, indent=2, ensure_ascii=False)
            
            print(f"üìä Rapport d√©taill√©: {rapport_file}")
            
            # R√©sum√© pour le jury
            print("\nüéØ R√âSUM√â FINAL:")
            print("=" * 60)
            print("‚úÖ TRACABILIT√â DES SOURCES D√âMONTR√âE:")
            
            # Compter les sources
            source_counts = {}
            for word, info in source_traceability.items():
                for source, count in info['sources'].items():
                    source_counts[source] = source_counts.get(source, 0) + count
            
            for source, count in source_counts.items():
                print(f"   ‚Ä¢ {source.upper()}: {count} contributions")
            
            print("\n‚úÖ COMP√âTENCES D√âMONTR√âES:")
            print("   ‚Ä¢ Tra√ßabilit√© compl√®te des donn√©es")
            print("   ‚Ä¢ Association mot-source en temps r√©el")
            print("   ‚Ä¢ Analyse multi-sources")
            print("   ‚Ä¢ Visualisation avec contexte")
            print("   ‚Ä¢ Conformit√© RGPD maintenue")
            
            print("\nüåê INTERFACES DISPONIBLES:")
            print("   ‚Ä¢ Streamlit: http://localhost:8501")
            print("   ‚Ä¢ API FastAPI: http://localhost:8000")
            print("   ‚Ä¢ Documentation: http://localhost:8000/docs")
            
        else:
            print("‚ùå √âchec de la g√©n√©ration du nuage de mots")
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    demo_wordcloud_with_sources()
