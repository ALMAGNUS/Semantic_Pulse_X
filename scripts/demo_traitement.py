#!/usr/bin/env python3
"""
Script de dÃ©monstration traitement - Semantic Pulse X
DÃ©monstration dÃ©taillÃ©e du traitement des donnÃ©es avec prints explicites
"""

import sys
import os
from pathlib import Path
import pandas as pd
import json

# Ajouter le rÃ©pertoire racine au PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.backend.core.anonymization import anonymizer

def demo_traitement_detaille():
    """DÃ©monstration dÃ©taillÃ©e du traitement des donnÃ©es"""
    print("ğŸ” DÃ‰MONSTRATION TRAITEMENT DÃ‰TAILLÃ‰ - Semantic Pulse X")
    print("=" * 70)
    
    # CrÃ©er des donnÃ©es de test
    print("\nğŸ“Š Ã‰tape 1: CrÃ©ation des donnÃ©es de test")
    print("-" * 50)
    
    test_data = [
        {
            "id": 1,
            "texte": "J'adore cette Ã©mission, c'est gÃ©nial !",
            "auteur": "user123",
            "date": "2024-01-15 20:30:00",
            "source": "twitter"
        },
        {
            "id": 2,
            "texte": "Cette Ã©mission est nulle, je dÃ©teste !",
            "auteur": "user456",
            "date": "2024-01-15 20:35:00",
            "source": "youtube"
        },
        {
            "id": 3,
            "texte": "Super Ã©pisode, j'ai hÃ¢te de voir la suite",
            "auteur": "user789",
            "date": "2024-01-15 20:40:00",
            "source": "instagram"
        }
    ]
    
    print(f"âœ… {len(test_data)} donnÃ©es de test crÃ©Ã©es")
    for i, data in enumerate(test_data, 1):
        print(f"   {i}. {data['texte'][:50]}... (auteur: {data['auteur']})")
    
    # Nettoyage
    print("\nğŸ§¹ Ã‰tape 2: Nettoyage des donnÃ©es")
    print("-" * 50)
    
    cleaned_data = []
    for data in test_data:
        print(f"ğŸ”„ Nettoyage de: '{data['texte']}'")
        
        # Supprimer les caractÃ¨res spÃ©ciaux
        texte_clean = data['texte'].replace('!', '').replace('?', '').strip()
        print(f"   â†’ Suppression caractÃ¨res spÃ©ciaux: '{texte_clean}'")
        
        # Normaliser la casse
        texte_normalise = texte_clean.lower()
        print(f"   â†’ Normalisation casse: '{texte_normalise}'")
        
        data_clean = data.copy()
        data_clean['texte'] = texte_normalise
        cleaned_data.append(data_clean)
        print(f"   âœ… DonnÃ©e nettoyÃ©e")
    
    print(f"âœ… {len(cleaned_data)} donnÃ©es nettoyÃ©es")
    
    # DÃ©doublonnage
    print("\nğŸ”„ Ã‰tape 3: DÃ©doublonnage")
    print("-" * 50)
    
    # Simuler un doublon
    cleaned_data.append(cleaned_data[0].copy())
    print(f"ğŸ“Š Avant dÃ©doublonnage: {len(cleaned_data)} donnÃ©es")
    
    # DÃ©doublonnage par texte
    seen_texts = set()
    deduplicated_data = []
    for data in cleaned_data:
        if data['texte'] not in seen_texts:
            seen_texts.add(data['texte'])
            deduplicated_data.append(data)
            print(f"   âœ… GardÃ©: '{data['texte']}'")
        else:
            print(f"   âŒ SupprimÃ© (doublon): '{data['texte']}'")
    
    print(f"âœ… AprÃ¨s dÃ©doublonnage: {len(deduplicated_data)} donnÃ©es")
    
    # Anonymisation RGPD
    print("\nğŸ”’ Ã‰tape 4: Anonymisation RGPD")
    print("-" * 50)
    
    anonymized_data = []
    for data in deduplicated_data:
        print(f"ğŸ”„ Anonymisation de: auteur='{data['auteur']}'")
        
        # Anonymiser l'auteur
        auteur_anonyme = anonymizer.anonymize_user_id(data['auteur'])
        print(f"   â†’ Auteur anonymisÃ©: '{auteur_anonyme}'")
        
        # Pseudonymiser le texte (hashage)
        texte_hash = anonymizer.hash_text(data['texte'])
        print(f"   â†’ Texte hashÃ©: '{texte_hash[:20]}...'")
        
        data_anonyme = data.copy()
        data_anonyme['auteur'] = auteur_anonyme
        data_anonyme['texte_hash'] = texte_hash
        anonymized_data.append(data_anonyme)
        print(f"   âœ… DonnÃ©e anonymisÃ©e")
    
    print(f"âœ… {len(anonymized_data)} donnÃ©es anonymisÃ©es")
    
    # HomogÃ©nÃ©isation
    print("\nğŸ“ Ã‰tape 5: HomogÃ©nÃ©isation des formats")
    print("-" * 50)
    
    homogenized_data = []
    for data in anonymized_data:
        print(f"ğŸ”„ HomogÃ©nÃ©isation de la donnÃ©e {data['id']}")
        
        # Standardiser le format de date
        from datetime import datetime
        date_obj = datetime.strptime(data['date'], '%Y-%m-%d %H:%M:%S')
        date_iso = date_obj.isoformat()
        print(f"   â†’ Date ISO: '{date_iso}'")
        
        # Ajouter des champs standardisÃ©s
        data_homogene = {
            'id': data['id'],
            'contenu': data['texte'],
            'contenu_hash': data['texte_hash'],
            'utilisateur_anonyme': data['auteur'],
            'timestamp': date_iso,
            'source_type': data['source'],
            'longueur': len(data['texte']),
            'mots': len(data['texte'].split())
        }
        
        homogenized_data.append(data_homogene)
        print(f"   âœ… DonnÃ©e homogÃ©nÃ©isÃ©e: {data_homogene['mots']} mots")
    
    print(f"âœ… {len(homogenized_data)} donnÃ©es homogÃ©nÃ©isÃ©es")
    
    # AgrÃ©gation multi-sources
    print("\nğŸ”— Ã‰tape 6: AgrÃ©gation multi-sources")
    print("-" * 50)
    
    # Grouper par source
    sources = {}
    for data in homogenized_data:
        source = data['source_type']
        if source not in sources:
            sources[source] = []
        sources[source].append(data)
    
    print("ğŸ“Š RÃ©partition par source:")
    for source, data_list in sources.items():
        print(f"   - {source}: {len(data_list)} donnÃ©es")
    
    # Statistiques globales
    total_mots = sum(data['mots'] for data in homogenized_data)
    longueur_moyenne = sum(data['longueur'] for data in homogenized_data) / len(homogenized_data)
    
    print(f"\nğŸ“ˆ Statistiques globales:")
    print(f"   - Total donnÃ©es: {len(homogenized_data)}")
    print(f"   - Total mots: {total_mots}")
    print(f"   - Longueur moyenne: {longueur_moyenne:.1f} caractÃ¨res")
    
    # Sauvegarde
    print("\nğŸ’¾ Ã‰tape 7: Sauvegarde des donnÃ©es traitÃ©es")
    print("-" * 50)
    
    output_file = project_root / "data" / "processed" / "donnees_traitees.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(homogenized_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… DonnÃ©es sauvegardÃ©es: {output_file}")
    print(f"   Taille: {output_file.stat().st_size} octets")
    
    print("\nğŸ‰ DÃ‰MONSTRATION TERMINÃ‰E !")
    print("=" * 70)
    print("âœ… Toutes les Ã©tapes de traitement ont Ã©tÃ© dÃ©montrÃ©es:")
    print("   1. CrÃ©ation des donnÃ©es de test")
    print("   2. Nettoyage (caractÃ¨res spÃ©ciaux, casse)")
    print("   3. DÃ©doublonnage (suppression des doublons)")
    print("   4. Anonymisation RGPD (hachage, pseudonymisation)")
    print("   5. HomogÃ©nÃ©isation (formats standardisÃ©s)")
    print("   6. AgrÃ©gation multi-sources (groupement)")
    print("   7. Sauvegarde (fichier JSON)")

if __name__ == "__main__":
    demo_traitement_detaille()
