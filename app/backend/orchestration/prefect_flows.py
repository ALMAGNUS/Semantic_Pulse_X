"""
Orchestration Prefect - Semantic Pulse X
Gestion des workflows et t√¢ches automatis√©es
"""

from datetime import datetime
from typing import Any

from prefect import flow, get_run_logger, task

from app.backend.ai.emotion_classifier import emotion_classifier
from app.backend.ai.langchain_agent import semantic_agent
from app.backend.ai.topic_clustering import topic_clustering
from app.backend.core.metrics import track_data_ingestion, track_emotion_processing
from app.backend.etl.pipeline import etl_pipeline


@task(name="extract_data", retries=3, retry_delay_seconds=60)
async def extract_data_task() -> dict[str, list[dict[str, Any]]]:
    """T√¢che d'extraction des donn√©es"""
    logger = get_run_logger()
    logger.info("üîÑ D√©but de l'extraction des donn√©es")

    try:
        # Ex√©cuter l'extraction
        raw_data = etl_pipeline._extract_data()

        # Tracker les m√©triques
        for source, data in raw_data.items():
            track_data_ingestion(source, "success")
            logger.info(f"‚úÖ {len(data)} enregistrements extraits depuis {source}")

        return raw_data

    except Exception as e:
        logger.error(f"‚ùå Erreur extraction: {e}")
        # Tracker l'erreur
        track_data_ingestion("unknown", "error")
        raise


@task(name="transform_data", retries=2, retry_delay_seconds=30)
async def transform_data_task(raw_data: dict[str, list[dict[str, Any]]]) -> dict[str, Any]:
    """T√¢che de transformation des donn√©es"""
    logger = get_run_logger()
    logger.info("üîÑ D√©but de la transformation des donn√©es")

    try:
        # Ex√©cuter la transformation
        processed_df = etl_pipeline._transform_data(raw_data)

        # Tracker les m√©triques
        track_data_ingestion("transformation", "success")
        logger.info(f"‚úÖ {len(processed_df)} enregistrements transform√©s")

        return {
            "processed_data": processed_df,
            "total_records": len(processed_df),
            "sources": processed_df['source_type'].value_counts().to_dict()
        }

    except Exception as e:
        logger.error(f"‚ùå Erreur transformation: {e}")
        track_data_ingestion("transformation", "error")
        raise


@task(name="ai_analysis", retries=2, retry_delay_seconds=45)
async def ai_analysis_task(processed_data: dict[str, Any]) -> dict[str, Any]:
    """T√¢che d'analyse IA"""
    logger = get_run_logger()
    logger.info("üß† D√©but de l'analyse IA")

    try:
        df = processed_data["processed_data"]
        texts = df['text'].tolist()

        # Classification √©motionnelle
        logger.info("üé≠ Classification √©motionnelle...")
        emotion_results = emotion_classifier.classify_batch(texts)

        # Tracker les m√©triques
        for result in emotion_results:
            track_emotion_processing(result['emotion_principale'], 'ai_analysis')

        # Clustering th√©matique
        logger.info("üìä Clustering th√©matique...")
        topic_results = topic_clustering.fit_topics(texts)

        # G√©n√©ration d'insights
        logger.info("üí° G√©n√©ration d'insights...")
        insights = semantic_agent.analyze_emotion_trends(emotion_results)

        ai_results = {
            "emotion_analysis": emotion_results,
            "topic_clustering": topic_results,
            "insights": insights,
            "total_processed": len(texts)
        }

        logger.info("‚úÖ Analyse IA termin√©e")
        return ai_results

    except Exception as e:
        logger.error(f"‚ùå Erreur analyse IA: {e}")
        raise


@task(name="load_data", retries=2, retry_delay_seconds=30)
async def load_data_task(processed_data: dict[str, Any], ai_results: dict[str, Any]) -> dict[str, Any]:
    """T√¢che de chargement des donn√©es"""
    logger = get_run_logger()
    logger.info("üíæ D√©but du chargement des donn√©es")

    try:
        # Ex√©cuter le chargement
        load_results = etl_pipeline._load_data(processed_data["processed_data"])

        # Sauvegarder les r√©sultats IA
        ai_path = etl_pipeline.processed_dir / "ai_analysis_results.parquet"
        processed_data["processed_data"].to_parquet(ai_path, index=False)

        # Tracker les m√©triques
        track_data_ingestion("loading", "success")

        logger.info("‚úÖ Donn√©es charg√©es avec succ√®s")
        return {
            "load_results": load_results,
            "ai_results": ai_results,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"‚ùå Erreur chargement: {e}")
        track_data_ingestion("loading", "error")
        raise


@task(name="generate_report", retries=1)
async def generate_report_task(
    raw_data: dict[str, list[dict[str, Any]]],
    processed_data: dict[str, Any],
    ai_results: dict[str, Any],
    load_results: dict[str, Any]
) -> dict[str, Any]:
    """T√¢che de g√©n√©ration du rapport"""
    logger = get_run_logger()
    logger.info("üìä G√©n√©ration du rapport final")

    try:
        # G√©n√©rer le rapport
        report = etl_pipeline._generate_report(raw_data, processed_data["processed_data"], ai_results)

        # Ajouter les r√©sultats de chargement
        report["load_results"] = load_results
        report["pipeline_status"] = "completed"
        report["timestamp"] = datetime.now().isoformat()

        logger.info("‚úÖ Rapport g√©n√©r√© avec succ√®s")
        return report

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
        raise


@flow(
    name="semantic_pulse_etl_flow",
    # task_runner=SequentialTaskRunner(),  # Supprim√© pour Prefect 2.x
    retries=1,
    retry_delay_seconds=300
)
async def semantic_pulse_etl_flow() -> dict[str, Any]:
    """Flow principal ETL de Semantic Pulse X"""
    logger = get_run_logger()
    logger.info("üöÄ D√©marrage du flow ETL Semantic Pulse X")

    try:
        # 1. Extraction
        raw_data = await extract_data_task()

        # 2. Transformation
        processed_data = await transform_data_task(raw_data)

        # 3. Analyse IA
        ai_results = await ai_analysis_task(processed_data)

        # 4. Chargement
        load_results = await load_data_task(processed_data, ai_results)

        # 5. Rapport
        report = await generate_report_task(raw_data, processed_data, ai_results, load_results)

        logger.info("‚úÖ Flow ETL termin√© avec succ√®s")
        return report

    except Exception as e:
        logger.error(f"‚ùå Erreur flow ETL: {e}")
        raise


@flow(name="emotion_analysis_flow")
async def emotion_analysis_flow(texts: list[str]) -> dict[str, Any]:
    """Flow d'analyse √©motionnelle en temps r√©el"""
    logger = get_run_logger()
    logger.info(f"üé≠ Analyse √©motionnelle de {len(texts)} textes")

    try:
        # Classification √©motionnelle
        emotion_results = emotion_classifier.classify_batch(texts)

        # Analyse des tendances
        trend_analysis = emotion_classifier.detect_emotion_trend(
            [(text, datetime.now().isoformat()) for text in texts]
        )

        # G√©n√©ration d'insights
        insights = semantic_agent.analyze_emotion_trends(emotion_results)

        # D√©tection d'anomalies
        anomalies = semantic_agent.detect_anomalies(emotion_results)

        result = {
            "emotion_results": emotion_results,
            "trend_analysis": trend_analysis,
            "insights": insights,
            "anomalies": anomalies,
            "total_texts": len(texts),
            "timestamp": datetime.now().isoformat()
        }

        logger.info("‚úÖ Analyse √©motionnelle termin√©e")
        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur analyse √©motionnelle: {e}")
        raise


@flow(name="topic_clustering_flow")
async def topic_clustering_flow(texts: list[str]) -> dict[str, Any]:
    """Flow de clustering th√©matique"""
    logger = get_run_logger()
    logger.info(f"üìä Clustering th√©matique de {len(texts)} textes")

    try:
        # Clustering
        topic_results = topic_clustering.fit_topics(texts)

        # Pr√©diction des topics
        topic_predictions = topic_clustering.predict_topics(texts)

        # Analyse de l'√©volution
        evolution = topic_clustering.get_topic_evolution(
            [(text, datetime.now().isoformat()) for text in texts]
        )

        result = {
            "topic_results": topic_results,
            "predictions": topic_predictions,
            "evolution": evolution,
            "total_texts": len(texts),
            "timestamp": datetime.now().isoformat()
        }

        logger.info("‚úÖ Clustering th√©matique termin√©")
        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur clustering: {e}")
        raise


@flow(name="monitoring_flow")
async def monitoring_flow() -> dict[str, Any]:
    """Flow de monitoring et alertes"""
    logger = get_run_logger()
    logger.info("üìà V√©rification du monitoring")

    try:
        # V√©rifier l'√©tat des services
        services_status = {
            "etl_pipeline": "healthy",
            "emotion_classifier": "healthy",
            "topic_clustering": "healthy",
            "langchain_agent": "healthy"
        }

        # V√©rifier les m√©triques
        metrics_status = {
            "data_ingestion_rate": "normal",
            "emotion_processing_time": "normal",
            "error_rate": "low",
            "memory_usage": "normal"
        }

        # G√©n√©rer des alertes si n√©cessaire
        alerts = []
        if metrics_status["error_rate"] == "high":
            alerts.append({
                "type": "error_rate_high",
                "message": "Taux d'erreur √©lev√© d√©tect√©",
                "severity": "warning"
            })

        result = {
            "services_status": services_status,
            "metrics_status": metrics_status,
            "alerts": alerts,
            "timestamp": datetime.now().isoformat()
        }

        logger.info("‚úÖ Monitoring v√©rifi√©")
        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur monitoring: {e}")
        raise


# D√©ploiements Prefect - COMMENT√â POUR PRECFECT 2.x
def create_deployments():
    """Cr√©e les d√©ploiements Prefect - D√âSACTIV√â pour Prefect 2.x"""
    logger = get_run_logger()
    logger.warning("D√©ploiements Prefect d√©sactiv√©s pour compatibilit√© Prefect 2.x")
    return []

    # Code comment√© pour Prefect 2.x
    """
    # D√©ploiement ETL quotidien
    etl_deployment = Deployment.build_from_flow(
        flow=semantic_pulse_etl_flow,
        name="semantic-pulse-etl-daily",
        schedule=CronSchedule(cron="0 2 * * *"),  # Tous les jours √† 2h
        work_pool_name="default-agent-pool"
    )

    # D√©ploiement analyse √©motionnelle (toutes les heures)
    emotion_deployment = Deployment.build_from_flow(
        flow=emotion_analysis_flow,
        name="emotion-analysis-hourly",
        schedule=CronSchedule(cron="0 * * * *"),  # Toutes les heures
        work_pool_name="default-agent-pool"
    )

    # D√©ploiement clustering (toutes les 6 heures)
    clustering_deployment = Deployment.build_from_flow(
        flow=topic_clustering_flow,
        name="topic-clustering-6h",
        schedule=CronSchedule(cron="0 */6 * * *"),  # Toutes les 6 heures
        work_pool_name="default-agent-pool"
    )

    # D√©ploiement monitoring (toutes les 15 minutes)
    monitoring_deployment = Deployment.build_from_flow(
        flow=monitoring_flow,
        name="monitoring-15min",
        schedule=CronSchedule(cron="*/15 * * * *"),  # Toutes les 15 minutes
        work_pool_name="default-agent-pool"
    )

    return [etl_deployment, emotion_deployment, clustering_deployment, monitoring_deployment]
    """


# Configuration Prefect
def configure_prefect():
    """Configure Prefect pour Semantic Pulse X"""
    from prefect import settings

    # Configuration de base
    settings.PREFECT_API_URL = "http://localhost:4200/api"
    settings.PREFECT_UI_URL = "http://localhost:4200"

    # Configuration des logs
    settings.PREFECT_LOGGING_LEVEL = "INFO"

    # Configuration des retries
    settings.PREFECT_TASK_DEFAULT_RETRY_DELAY_SECONDS = 30
    settings.PREFECT_TASK_DEFAULT_RETRIES = 2


if __name__ == "__main__":
    # Configuration
    configure_prefect()

    # Cr√©er les d√©ploiements
    deployments = create_deployments()

    # Appliquer les d√©ploiements
    for deployment in deployments:
        deployment.apply()

    print("‚úÖ D√©ploiements Prefect cr√©√©s avec succ√®s")
