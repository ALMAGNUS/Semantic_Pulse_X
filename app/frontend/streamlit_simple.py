#!/usr/bin/env python3
"""
Streamlit simplifi√© - Semantic Pulse X
Focus sur les nuages de mots et visualisations
"""

import streamlit as st
import sys
import json
from pathlib import Path

# Ajouter le r√©pertoire racine au PYTHONPATH
project_root = Path(__file__).parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Imports simplifi√©s
try:
    from app.backend.ai.emotion_classifier import emotion_classifier
    from app.backend.ai.topic_clustering import topic_clustering
    from app.backend.core.anonymization import anonymizer
    from app.frontend.visualization.wordcloud_generator import wordcloud_generator
except ImportError as e:
    st.error(f"Erreur d'import: {e}")
    st.stop()

def main():
    """Application Streamlit principale"""
    
    st.set_page_config(
        page_title="Semantic Pulse X - Nuages de Mots",
        page_icon="üåä",
        layout="wide"
    )
    
    st.title("üåä Semantic Pulse X - Nuages de Mots")
    st.markdown("### Visualisation des vagues √©motionnelles m√©diatiques")
    
    # Sidebar
    st.sidebar.title("üéõÔ∏è Configuration")
    
    # Param√®tres des nuages de mots
    emotion = st.sidebar.selectbox(
        "√âmotion √† analyser",
        ["joy", "sadness", "anger", "fear", "surprise", "disgust"],
        index=0
    )
    
    max_words = st.sidebar.slider("Nombre maximum de mots", 10, 100, 30)
    
    colormap = st.sidebar.selectbox(
        "Palette de couleurs",
        ["viridis", "plasma", "inferno", "magma", "coolwarm", "RdYlBu"],
        index=0
    )
    
    # Bouton de g√©n√©ration
    if st.sidebar.button("üé® G√©n√©rer le nuage de mots", type="primary"):
        with st.spinner("G√©n√©ration du nuage de mots..."):
            try:
                # Charger les donn√©es avec sources
                data_file = Path("data/processed/donnees_traitees_demo.json")
                if data_file.exists():
                    with open(data_file, 'r', encoding='utf-8') as f:
                        donnees = json.load(f)
                    
                    # Pr√©parer les donn√©es avec sources
                    data_with_sources = []
                    for data in donnees:
                        data_with_sources.append({
                            'text': data['contenu'],
                            'source': data['source_type'],
                            'emotion': emotion,
                            'user_id': data['utilisateur_anonyme']
                        })
                    
                    # G√©n√©rer le nuage de mots avec tra√ßabilit√©
                    wordcloud_data = wordcloud_generator.generate_emotion_wordcloud_with_sources(
                        data=data_with_sources,
                        emotion_filter=emotion,
                        max_words=max_words,
                        width=800,
                        height=400
                    )
                else:
                    st.error("‚ùå Aucune donn√©e trouv√©e. Lancez d'abord le script de d√©monstration.")
                    st.stop()
                
                if wordcloud_data:
                    st.success("‚úÖ Nuage de mots g√©n√©r√© avec succ√®s !")
                    
                    # Afficher le nuage de mots
                    st.image(wordcloud_data['image'], caption=f"Nuage de mots - √âmotion: {emotion}", use_column_width=True)
                    
                    # Statistiques
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("√âmotion", emotion.title())
                    with col2:
                        st.metric("Mots max", max_words)
                    with col3:
                        st.metric("Palette", colormap)
                    
                    # Tra√ßabilit√© des sources
                    if 'source_traceability' in wordcloud_data:
                        st.markdown("### üîç Tra√ßabilit√© des Sources")
                        
                        source_traceability = wordcloud_data['source_traceability']
                        
                        # Afficher les mots les plus fr√©quents avec leurs sources
                        for word, info in list(source_traceability.items())[:10]:
                            with st.expander(f"üìù '{word}' (fr√©quence: {info['frequency']})"):
                                st.write(f"**Source principale:** {info['main_source'].upper()}")
                                st.write("**R√©partition par source:**")
                                
                                for source, count in info['sources'].items():
                                    percentage = (count / info['frequency']) * 100
                                    st.write(f"‚Ä¢ {source.upper()}: {count} fois ({percentage:.1f}%)")
                        
                else:
                    st.error("‚ùå Erreur lors de la g√©n√©ration du nuage de mots")
                    
            except Exception as e:
                st.error(f"‚ùå Erreur: {e}")
    
    # Section de d√©monstration
    st.markdown("---")
    st.markdown("## üìä D√©monstration Data Engineering")
    
    if st.button("üîç Voir les r√©sultats de traitement"):
        try:
            # Charger les donn√©es trait√©es
            data_file = Path("data/processed/donnees_traitees_demo.json")
            if data_file.exists():
                with open(data_file, 'r', encoding='utf-8') as f:
                    donnees = json.load(f)
                
                st.success(f"‚úÖ {len(donnees)} donn√©es trait√©es charg√©es")
                
                # Afficher les donn√©es
                for i, data in enumerate(donnees, 1):
                    with st.expander(f"Donn√©e {i}: {data['contenu'][:50]}..."):
                        st.write(f"**Texte:** {data['contenu']}")
                        st.write(f"**Utilisateur anonymis√©:** {data['utilisateur_anonyme']}")
                        st.write(f"**Source:** {data['source_type']}")
                        st.write(f"**Mots:** {data['nombre_mots']}")
                        st.write(f"**Caract√®res:** {data['longueur_caracteres']}")
                        st.write(f"**Densit√©:** {data['densite_mots']}%")
            else:
                st.warning("‚ö†Ô∏è Aucune donn√©e trait√©e trouv√©e. Lancez d'abord le script de d√©monstration.")
                
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")
    
    # Section API
    st.markdown("---")
    st.markdown("## üåê API FastAPI")
    
    if st.button("üîó Tester l'API"):
        try:
            import requests
            
            # Test de l'API
            response = requests.get("http://localhost:8000/health", timeout=5)
            if response.status_code == 200:
                st.success("‚úÖ API FastAPI accessible")
                st.json(response.json())
            else:
                st.error(f"‚ùå API non accessible: {response.status_code}")
                
        except requests.exceptions.ConnectionError:
            st.error("‚ùå API FastAPI non lanc√©e. Lancez: `python -m uvicorn app.backend.main:app --reload --port 8000`")
        except Exception as e:
            st.error(f"‚ùå Erreur: {e}")
    
    # Footer
    st.markdown("---")
    st.markdown("### üéØ Semantic Pulse X - Pipeline de Data Engineering RGPD")
    st.markdown("**Comp√©tences d√©montr√©es:** Nettoyage, D√©doublonnage, Anonymisation, Homog√©n√©isation, Visualisation")

if __name__ == "__main__":
    main()
