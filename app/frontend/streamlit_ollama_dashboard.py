"""
Dashboard Ollama - Semantic Pulse X
Interface de gestion des modÃ¨les IA locaux
"""

import streamlit as st
import requests
import json
from typing import List, Dict, Any
from datetime import datetime
import time

from app.backend.ai.ollama_client import ollama_client


def show_ollama_dashboard():
    """Affiche le dashboard Ollama"""
    st.header("ğŸ¤– Dashboard Ollama - ModÃ¨les IA Locaux")
    
    # VÃ©rifier le statut d'Ollama
    if not ollama_client.is_available():
        st.error("âŒ Ollama n'est pas disponible!")
        st.info("ğŸ’¡ DÃ©marrez Ollama avec: `docker-compose -f docker-compose.ollama.yml up -d`")
        return
    
    # Statut gÃ©nÃ©ral
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Statut Ollama", "ğŸŸ¢ Actif")
    
    with col2:
        models = ollama_client.get_available_models()
        st.metric("ModÃ¨les disponibles", len(models))
    
    with col3:
        st.metric("ModÃ¨le par dÃ©faut", "mistral:7b")
    
    # Onglets
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“‹ ModÃ¨les", 
        "ğŸ§ª Test", 
        "ğŸ’¬ Chat", 
        "âš™ï¸ Configuration"
    ])
    
    with tab1:
        show_models_tab()
    
    with tab2:
        show_test_tab()
    
    with tab3:
        show_chat_tab()
    
    with tab4:
        show_config_tab()


def show_models_tab():
    """Onglet des modÃ¨les"""
    st.subheader("ğŸ“‹ ModÃ¨les disponibles")
    
    models = ollama_client.get_available_models()
    
    if not models:
        st.warning("Aucun modÃ¨le disponible")
        return
    
    # Liste des modÃ¨les
    for model in models:
        with st.expander(f"ğŸ¤– {model}"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Nom:** {model}")
                st.write(f"**Statut:** âœ… Disponible")
            
            with col2:
                if st.button(f"Tester {model}", key=f"test_{model}"):
                    test_model(model)
    
    # Recommandations
    st.subheader("ğŸ’¡ Recommandations")
    
    recommended_models = [
        {"name": "mistral:7b", "description": "Excellent pour le franÃ§ais, recommandÃ© pour Semantic Pulse"},
        {"name": "llama2:7b", "description": "Polyvalent et performant, bon pour l'anglais"},
        {"name": "codellama:7b", "description": "SpÃ©cialisÃ© dans la gÃ©nÃ©ration de code"},
        {"name": "phi3:3.8b", "description": "LÃ©ger et rapide, idÃ©al pour les tests"}
    ]
    
    for rec in recommended_models:
        status = "âœ…" if rec["name"] in models else "âŒ"
        st.write(f"{status} **{rec['name']}** - {rec['description']}")


def show_test_tab():
    """Onglet de test"""
    st.subheader("ğŸ§ª Test des modÃ¨les")
    
    models = ollama_client.get_available_models()
    
    if not models:
        st.warning("Aucun modÃ¨le disponible pour les tests")
        return
    
    # SÃ©lection du modÃ¨le
    selected_model = st.selectbox("SÃ©lectionner un modÃ¨le", models)
    
    # Prompt de test
    test_prompt = st.text_area(
        "Prompt de test",
        value="Analysez les tendances Ã©motionnelles dans les mÃ©dias et fournissez des insights.",
        height=100
    )
    
    # ParamÃ¨tres
    col1, col2 = st.columns(2)
    
    with col1:
        temperature = st.slider("TempÃ©rature", 0.0, 1.0, 0.7, 0.1)
    
    with col2:
        max_tokens = st.slider("Max tokens", 100, 2000, 1000, 100)
    
    # Bouton de test
    if st.button("ğŸš€ Tester le modÃ¨le", type="primary"):
        with st.spinner("Test en cours..."):
            start_time = time.time()
            
            try:
                response = ollama_client.generate_text(
                    prompt=test_prompt,
                    model=selected_model,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                end_time = time.time()
                duration = end_time - start_time
                
                # Afficher les rÃ©sultats
                st.success(f"âœ… Test rÃ©ussi en {duration:.2f}s")
                
                st.subheader("ğŸ“ RÃ©ponse du modÃ¨le:")
                st.write(response)
                
                # MÃ©triques
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("DurÃ©e", f"{duration:.2f}s")
                with col2:
                    st.metric("Tokens gÃ©nÃ©rÃ©s", len(response.split()))
                with col3:
                    st.metric("ModÃ¨le", selected_model)
                
            except Exception as e:
                st.error(f"âŒ Erreur lors du test: {e}")


def show_chat_tab():
    """Onglet de chat"""
    st.subheader("ğŸ’¬ Chat avec l'IA")
    
    models = ollama_client.get_available_models()
    
    if not models:
        st.warning("Aucun modÃ¨le disponible pour le chat")
        return
    
    # SÃ©lection du modÃ¨le
    selected_model = st.selectbox("ModÃ¨le de chat", models, key="chat_model")
    
    # Initialiser l'historique de chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Afficher l'historique
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # Input utilisateur
    user_input = st.chat_input("Posez une question Ã  l'IA...")
    
    if user_input:
        # Ajouter le message utilisateur
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        
        # GÃ©nÃ©rer la rÃ©ponse
        with st.spinner("L'IA rÃ©flÃ©chit..."):
            try:
                response = ollama_client.generate_text(
                    prompt=user_input,
                    model=selected_model,
                    temperature=0.7
                )
                
                # Ajouter la rÃ©ponse
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                
                # Afficher la rÃ©ponse
                with st.chat_message("assistant"):
                    st.write(response)
                    
            except Exception as e:
                error_msg = f"Erreur: {str(e)}"
                st.session_state.chat_history.append({"role": "assistant", "content": error_msg})
                
                with st.chat_message("assistant"):
                    st.write(error_msg)
    
    # Bouton pour effacer l'historique
    if st.button("ğŸ—‘ï¸ Effacer l'historique"):
        st.session_state.chat_history = []
        st.rerun()


def show_config_tab():
    """Onglet de configuration"""
    st.subheader("âš™ï¸ Configuration Ollama")
    
    # Informations systÃ¨me
    st.subheader("ğŸ“Š Informations systÃ¨me")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("URL Ollama", "http://localhost:11434")
        st.metric("ModÃ¨les disponibles", len(ollama_client.get_available_models()))
    
    with col2:
        st.metric("Statut", "ğŸŸ¢ ConnectÃ©")
        st.metric("DerniÃ¨re vÃ©rification", datetime.now().strftime("%H:%M:%S"))
    
    # Configuration des modÃ¨les
    st.subheader("ğŸ”§ Configuration des modÃ¨les")
    
    st.info("ğŸ’¡ Pour installer de nouveaux modÃ¨les, utilisez le script: `python scripts/setup_ollama.py`")
    
    # Boutons d'action
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ”„ Actualiser les modÃ¨les"):
            st.rerun()
    
    with col2:
        if st.button("ğŸ§ª Test rapide"):
            test_quick()
    
    with col3:
        if st.button("ğŸ“Š Rapport de statut"):
            generate_status_report()


def test_model(model_name: str):
    """Teste un modÃ¨le spÃ©cifique"""
    with st.spinner(f"Test de {model_name}..."):
        try:
            response = ollama_client.generate_text(
                prompt="Bonjour, comment allez-vous?",
                model=model_name,
                temperature=0.7
            )
            
            st.success(f"âœ… {model_name} fonctionne correctement!")
            st.write(f"**RÃ©ponse:** {response}")
            
        except Exception as e:
            st.error(f"âŒ Erreur avec {model_name}: {e}")


def test_quick():
    """Test rapide de tous les modÃ¨les"""
    models = ollama_client.get_available_models()
    
    if not models:
        st.warning("Aucun modÃ¨le disponible")
        return
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results = []
    
    for i, model in enumerate(models):
        status_text.text(f"Test de {model}...")
        
        try:
            response = ollama_client.generate_text(
                prompt="Test rapide",
                model=model,
                temperature=0.1
            )
            results.append({"model": model, "status": "âœ… OK", "response": response[:50] + "..."})
        except Exception as e:
            results.append({"model": model, "status": "âŒ Erreur", "response": str(e)[:50] + "..."})
        
        progress_bar.progress((i + 1) / len(models))
    
    status_text.text("Test terminÃ©!")
    
    # Afficher les rÃ©sultats
    st.subheader("ğŸ“Š RÃ©sultats du test rapide")
    
    for result in results:
        st.write(f"{result['status']} **{result['model']}** - {result['response']}")


def generate_status_report():
    """GÃ©nÃ¨re un rapport de statut"""
    models = ollama_client.get_available_models()
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "ollama_status": "connected" if ollama_client.is_available() else "disconnected",
        "available_models": models,
        "total_models": len(models)
    }
    
    st.subheader("ğŸ“Š Rapport de statut")
    st.json(report)
    
    # Bouton de tÃ©lÃ©chargement
    st.download_button(
        label="ğŸ’¾ TÃ©lÃ©charger le rapport",
        data=json.dumps(report, indent=2, ensure_ascii=False),
        file_name=f"ollama_status_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )


if __name__ == "__main__":
    st.set_page_config(
        page_title="Ollama Dashboard",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    show_ollama_dashboard()
